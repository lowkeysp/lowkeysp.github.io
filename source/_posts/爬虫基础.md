---
title: 爬虫基础
date: 2019-09-16 20:26:00
tags: [爬虫]
categories: 爬虫
---


# scrapy安装

Scrapy是一个基于Twisted框架的使用python的开源网络爬虫框架

使用pip安装
```
pip install scrapy
```

确认是否安装成功,在pyhton环境中，输入
```
import scrapy
>>> scrapy.version_info
```


# 创建项目
使用`scrapy startproject`命令
```
scrapy startproject example
```
# scrapy框架

![捕获.PNG](http://ww1.sinaimg.cn/large/006eDJDNly1g78imf5vi7j31550pwq53.jpg)


首先，了解一下Scrapy框架的各个组件

* ENGINE：引擎，框架的核心，其他所有组件在其控制下协同工作，是内部组件
* SCHEDULER：调度器，负责对SPIDER提交的下载请求进行调度，是内部组件
* DOWNLOADER：下载器，负责下载页面（发送HTTP请求/接收HTTP请求），是内部组件
* SPIDER：爬虫，负责提取页面中的数据，并产生对新页面的下载请求，需要用户实现
* MIDDLEWARE：中间件，负责对Request对象和Response对象进行处理，是可选组件
* ITEM PIPELINE：数据管道，负责对爬取到的数据进行处理，是可选组件


我们来说明一下流程：
> 1. ENGINE从SPIDER拿到需要爬虫的Requests
> 2. ENGINE会在SCHEDULER里面对Requests进行调度，然后还会向SCHEDULER请求下一个要爬虫的Requests（也就是即将要爬虫的Requests）
> 3. SCHEDULER会将下一个要爬虫的Requests返回给ENGINE
> 4. ENGINE会将这个Requests传递给DOWNLOADER，其中需要经过DOWNLOADER的MIDDLEWARE（在process_request()中）
> 5. 一旦DOWNLOADER下载好了页面，DOWNLOADER会把相应的Response返回给ENGINE，同样需要经过中间件（在process_response()中）
> 6. ENGINE接收到从DOWNLOADER发过来的Response后，会发送给Spider用于处理，中间会经过Spider Middleware (在process_spider_input()中).
> 7. Spider会处理从ENGINE返回回来的Response，然后将爬下来的数据和新的Requests再次发送给ENGINE。中间会经过Spider Middleware (在process_spider_output()中).
> 8. ENGINE会将处理好的Items发送给Item PIPELINE，然后将新的Requests发送给SCHEDULER，并向SCHEDULER请求下一个要爬虫的Request
> 9. 一直这么重复，直到SCHEDULER中没有剩余的Requests

# Selector
当你在爬网页的时候，会经常需要从HTML中提取数据，也有一些现成的python库完成这些，比如BeautifulSoup和lxml。

Scrapy有自己的一套机制来提取数据，叫做Selector，通过`Xpath`或者`CSS`来选择数据

Xpath是一种在XML中选择nodes的语言，而CSS是将style应用到HTML文本的语言。

## 创建
Response对象通过`.selector`属性可以返回一个Selector对象
比如：
```
>>> response.selector.xpath('//span/text()').get()
'good'
```
由于我们会大量的使用`Xpath`和`CSS`，因此可以简化成`response.xpath()`和`response.css()`
```
>>> response.xpath('//span/text()').get()
'good'
>>> response.css('span::text').get()
'good'
```

通常我们都是使用上面的两种情况，如果要直接创建Selector的话，可以使用下面：

使用text创建
```
>>> from scrapy.selector import Selector
>>> body = '<html><body><span>good</span></body></html>'
>>> Selector(text=body).xpath('//span/text()').get()
'good'
```

使用response创建：
```
>>> from scrapy.selector import Selector
>>> from scrapy.http import HtmlResponse
>>> response = HtmlResponse(url='http://example.com', body=body)
>>> Selector(response=response).xpath('//span/text()').get()
'good'
```


## 使用Selector

我们先贴出整个HTML代码
```
<html>
 <head>
  <base href='http://example.com/' />
  <title>Example website</title>
 </head>
 <body>
  <div id='images'>
   <a href='image1.html'>Name: My image 1 <br /><img src='image1_thumb.jpg' /></a>
   <a href='image2.html'>Name: My image 2 <br /><img src='image2_thumb.jpg' /></a>
   <a href='image3.html'>Name: My image 3 <br /><img src='image3_thumb.jpg' /></a>
   <a href='image4.html'>Name: My image 4 <br /><img src='image4_thumb.jpg' /></a>
   <a href='image5.html'>Name: My image 5 <br /><img src='image5_thumb.jpg' /></a>
  </div>
 </body>
</html>
```


让我们来学习selector

选择title标签里面的text
```
 response.xpath('//title/text()')
[<Selector xpath='//title/text()' data='Example website'>]
```
为了能够得到文本数据，必须使用selector的`.get()`或者`.getall()`方法，如下：
```
>>> response.xpath('//title/text()').getall()
['Example website']
>>> response.xpath('//title/text()').get()
'Example website'
```
可以看出，`.get()`只会返回一个结果，如果有好几个结果的话，则会返回第一个；而`.getall()`会返回一个结果的list。

通常，使用CSS3 pseudo-elements，CSS Selector可以选择text或者attribute的node
```
>>> response.css('title::text').get()
'Example website'
```

从上面可以看出，`.xpath()`和`.css()`方法会返回一个SelectorList，里面包括了一些新的Selector，因此，我们可以连着使用，比如：
```
>>> response.css('img').xpath('@src').getall()
['image1_thumb.jpg',
 'image2_thumb.jpg',
 'image3_thumb.jpg',
 'image4_thumb.jpg',
 'image5_thumb.jpg']
```

如果你只是想获取匹配到的元素中的第一个的话，可以使用selector的`.get()`（或者`.extract_first()`）
```
>>> response.xpath('//div[@id="images"]/a/text()').get()
'Name: My image 1 
```
如果没有匹配的话，则会返回`None`。我们还可以使用其他的来代替这个返回的None值，比如：
```
>>> response.xpath('//div[@id="not-exists"]/text()').get(default='not-found')
'not-found'
```










