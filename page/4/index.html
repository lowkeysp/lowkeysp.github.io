<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="lowkeysp">
<meta property="og:type" content="website">
<meta property="og:title" content="lowkeysp&#39; Blog">
<meta property="og:url" content="http://example.com/page/4/index.html">
<meta property="og:site_name" content="lowkeysp&#39; Blog">
<meta property="og:description" content="lowkeysp">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="lowkeysp">
<meta property="article:tag" content="IT">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/4/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>lowkeysp' Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">lowkeysp' Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Stay Hungry, Stay Foolish!</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/27/%E5%88%9A%E5%AE%89%E8%A3%85%E7%9A%84ubuntu%E7%B3%BB%E7%BB%9Froot%E5%AF%86%E7%A0%81%E8%AE%BE%E7%BD%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="lowkeysp">
      <meta itemprop="description" content="lowkeysp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="lowkeysp' Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/27/%E5%88%9A%E5%AE%89%E8%A3%85%E7%9A%84ubuntu%E7%B3%BB%E7%BB%9Froot%E5%AF%86%E7%A0%81%E8%AE%BE%E7%BD%AE/" class="post-title-link" itemprop="url">刚安装的ubuntu系统root密码设置</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-11-27 09:15:05" itemprop="dateCreated datePublished" datetime="2020-11-27T09:15:05+08:00">2020-11-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-05-11 17:25:03" itemprop="dateModified" datetime="2023-05-11T17:25:03+08:00">2023-05-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>如果是刚安装的Ubuntu系统的话，root密码默认是随机的，这时候可以对root密码进行更改，<br>命令为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo passwd</span><br></pre></td></tr></table></figure>
<p>然后就可以按照提示设置新的密码</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/26/git-for-windows-%E9%95%9C%E5%83%8F%E5%9C%B0%E5%9D%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="lowkeysp">
      <meta itemprop="description" content="lowkeysp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="lowkeysp' Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/26/git-for-windows-%E9%95%9C%E5%83%8F%E5%9C%B0%E5%9D%80/" class="post-title-link" itemprop="url">git for windows 镜像地址</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-11-26 16:32:13" itemprop="dateCreated datePublished" datetime="2020-11-26T16:32:13+08:00">2020-11-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-05-11 17:25:03" itemprop="dateModified" datetime="2023-05-11T17:25:03+08:00">2023-05-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>git for windows 官网下载太慢了，在网上找到了一个镜像，速度很快，记录一下</p>
<p><a target="_blank" rel="noopener" href="https://npm.taobao.org/mirrors/git-for-windows/">https://npm.taobao.org/mirrors/git-for-windows/</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/31/%E4%BA%8C%E5%B1%82VPN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="lowkeysp">
      <meta itemprop="description" content="lowkeysp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="lowkeysp' Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/08/31/%E4%BA%8C%E5%B1%82VPN/" class="post-title-link" itemprop="url">二层VPN</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-08-31 08:43:18" itemprop="dateCreated datePublished" datetime="2020-08-31T08:43:18+08:00">2020-08-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-05-11 17:25:03" itemprop="dateModified" datetime="2023-05-11T17:25:03+08:00">2023-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E9%80%9A/" itemprop="url" rel="index"><span itemprop="name">数通</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <meta name="referrer" content="no-referrer" />


<p>MPLS L2VPN提供基于MPLS网络的二层VPN服务，包括<strong>点到点的VPWS业务</strong>（Virtual Private Wire Service）跟<strong>点到多点的VPLS业务</strong>（Virtual Private LAN Service）</p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gi9pfws5u8j31810je7fn.jpg" alt="捕获.PNG"></p>
<p>MPLS L2VPN就是在MPLS网络上透明传递用户的二层数据。从用户的角度来看，这个MPLS网络就是一个二层的交换网络，通过这个网络，可以在不同站点之间建立二层的连接</p>
<h1 id="VPWS"><a href="#VPWS" class="headerlink" title="VPWS"></a>VPWS</h1><h2 id="VPWS基本架构"><a href="#VPWS基本架构" class="headerlink" title="VPWS基本架构"></a>VPWS基本架构</h2><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gi9phz8sg5j30zy0bgq98.jpg" alt="捕获.PNG"></p>
<ul>
<li>接入电路AC（Attachment Circuit）：一条连接CE和PE的独立的链路或电路。AC接口可以是物理接口或逻辑接口。</li>
<li>虚电路VC（Virtual Circuit）：两个PE节点之间的一种逻辑连接。</li>
<li>隧道Tunnel（MPLS Tunnel）：用于在PE之间透明地传输用户数据。</li>
</ul>
<blockquote>
<p>VC提供用户二层数据穿越运营商骨干网络的通道，可以将其简单地理解为连接两个AC接口虚拟线路（点到点连接），将两条用户侧的AC“短接”起来。因此在MPLS L2VPN的实现中，VC又被称为PW（Pseudo Wire，伪线）。</p>
</blockquote>
<h2 id="VPWS-报文发送过程"><a href="#VPWS-报文发送过程" class="headerlink" title="VPWS 报文发送过程"></a>VPWS 报文发送过程</h2><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gi9pmfnnvpj31ac08sdjo.jpg" alt="捕获.PNG"></p>
<ul>
<li>MPLS L2VPN通过标签栈实现用户报文在MPLS网络中的透明传送</li>
<li>外层标签（称为Tunnel标签）用于将报文从一个PE传递到另一个PE。</li>
<li>内层标签（在MPLS L2VPN中称为VC标签）用于区分不同VPN中的不同连接，接收方PE根据VC标签决定将报文转发给哪个CE（哪个AC接口）。</li>
</ul>
<blockquote>
<p>华为查询方式：</p>
<p>display mpls l2vc interface XXX ：能查询内层VC标签，查询到VC tunnel下的tunnel id</p>
<p>display tunnel-info tunnel-id X ： 能查询出接口、外层标签</p>
</blockquote>
<h2 id="VPWS实现方式"><a href="#VPWS实现方式" class="headerlink" title="VPWS实现方式"></a>VPWS实现方式</h2><h3 id="Martini"><a href="#Martini" class="headerlink" title="Martini"></a>Martini</h3><p>Martini是基于LDP的，Martini方式使用两层标签，内层标签是采用扩展的LDP作为信令进行交互。</p>
<p>PE之间建立LDP的remote session，PE为CE之间的每条连接分配一个VC标签。二层VPN信息将携带着VC标签，通过LDP建立的LSP转发到remote session的对端PE。</p>
<p>在Martini方式中，两个CE间的VC Type + VC ID来识别一个VC。同一个VC Type的所有VC中，其VC ID必须在整个MPLS网络中唯一。</p>
<ul>
<li>VC-Type：表明VC的封装类型，例如ATM、VLAN或PPP。</li>
<li>VC-ID：标识VC。相同VC-Type的所有VC，其VC-ID必须在整个MPLS网络中唯一。</li>
</ul>
<h3 id="Martini的标签协商-VLL技术"><a href="#Martini的标签协商-VLL技术" class="headerlink" title="Martini的标签协商-VLL技术"></a>Martini的标签协商-VLL技术</h3><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gia20woay1j30wt07xdgk.jpg" alt="捕获.PNG"></p>
<p>在PE1上配置：</p>
<p>首先在PE之间配置 mpls ldp remote 会话</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mpls ldp remote-peer PE2</span><br><span class="line">    remote ip 3.0.0.42</span><br></pre></td></tr></table></figure>
<p>之后在PE1接入CE的接口上创建L2VPN连接</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mpls l2vc 3.0.0.42 1001</span><br></pre></td></tr></table></figure>

<p>同样的，在PE2上配置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mpls ldp remote-peer PE1</span><br><span class="line">    remote ip 3.0.0.44</span><br><span class="line"></span><br><span class="line">mpls l2vc 3.0.0.45 1001</span><br></pre></td></tr></table></figure>


<p>这样，PE1和PE2之间就建立了一条VC（也就是PW），VC_ID为1001</p>
<p>之后，先在PE1上的GE子接口上绑定L2VC，之后PE1会为该VC分配一个内层标签140288，放入mapping消息中，并发送request和mapping消息给PE2，由于PE2并没有配置L2VC，所以将不做回应</p>
<p>然后，在PE2上的GE子接口上配置L2VC，此时PE2会为该VC分配一个标签140290，将这个标签放在mapping消息中，并发送mapping和request报文给PE1。PE1接收到PE2发送过来的request和mapping消息后，作出反应，将自己分配的标签140288放 入mapping消息中，并发送mapping消息给PE2，再检查PE2发过来的mapping消息发现VC信息符合，便将mapping里面的标签140290放入本地，作为PE1的Remote Label，并使本地的VC状态up，如下图示。</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gia44kmgwrj30x109g3za.jpg" alt="捕获.PNG"></p>
<p>PE2接收到PE1端过来的mapping报文，比较之后发现VC信息相符，则将mapping中的标签140288放入自己本地，作为PE2的Remote Label，将VC的状态改为UP，此时VC建立完毕，如上图示。</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gia45j344ej30vs08vwf9.jpg" alt="捕获.PNG"></p>
<h3 id="Martini的转发过程"><a href="#Martini的转发过程" class="headerlink" title="Martini的转发过程"></a>Martini的转发过程</h3><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gia46vkjmej30wg0bvq3u.jpg" alt="捕获.PNG"></p>
<p>报文从CE1到达PE1，根据接口转发表查得公网标签（1027）和VC标签(140290)，打上标签从出接口转发出去。倒数第二跳的时候弹出标签，在PE2上查找VC标签对应的出接口，弹出私网标签，转发到CE2。</p>
<h3 id="一些命令"><a href="#一些命令" class="headerlink" title="一些命令"></a>一些命令</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Display mpls ldp session</span><br><span class="line"></span><br><span class="line">Display mpls ldp peer</span><br><span class="line"></span><br><span class="line">Display mpls l2vc brief</span><br><span class="line"></span><br><span class="line">Display mpls l2vc interface xxxxxxx</span><br><span class="line"></span><br><span class="line">display tunnel-info tunnel-id xxx</span><br><span class="line"></span><br><span class="line">display mpls l2vc vc-id</span><br></pre></td></tr></table></figure>



<h3 id="Kompella"><a href="#Kompella" class="headerlink" title="Kompella"></a>Kompella</h3><p>Kompella是另一种VLL技术的主流模式，与MPLS L3VPN很相似，特别是CE、PE、ROUTE-TARGET、RD、SITE的定义以及用途</p>
<p>区别是kompella传送的是二层信息，而MPLS BGP VPN传送的是三层路由信息，为此kompella进行了相应的BGP NLRI扩展</p>
<p>Kompella方式采取标签块的方式，一次为多个连接分配标签。用户可以指定一个本地CE的范围（CE range），表明这个CE能与多少个CE建立连接。系统会一次为这个CE分配一个标签块，标签块的大小等于CE range。这种方式允许用户为VPN分配一些额外的标签，留待以后使用。这样会造成标签资源的浪费，但是同时带来一个很大的好处：减少VPN部署和扩容时的配置工作量。</p>
<h4 id="标签块"><a href="#标签块" class="headerlink" title="标签块"></a>标签块</h4><ul>
<li>Kompella方式采用标签块（Label Block）分配标签，一次为多个连接分配标签。</li>
</ul>
<p>标签块的三要素：</p>
<ul>
<li>标签块的起始标签LB（Label Base）：由系统分配，表明该标签块的起始标签。</li>
<li>标签块的大小LR（Label Range）：用户可以指定一个本地CE的范围（CE range），表明这个CE能与多少个CE建立连接。系统一次为CE分配一个大小等于CE range的标签块。</li>
<li>偏移量LO（Label-block Offset）：标签不够用时，为了不破坏原有的VC连接，需要给CE分配一个新的标签块。多个标签之间的关系通过LO来定义，标志前面所有已分配的标签块大小的总数。</li>
</ul>
<h4 id="配置VC连接"><a href="#配置VC连接" class="headerlink" title="配置VC连接"></a>配置VC连接</h4><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gilo6nv22oj30rt07aq5v.jpg" alt="捕获.PNG"></p>
<p>进入MPLS-L2VPN视图：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mpls l2vpn l2vpn-name</span><br></pre></td></tr></table></figure>
<p>创建CE，并进入MPLS-L2VPN-CE视图</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ce ce-name id ce-id [range ce-range] [default-offset ce-offset ]</span><br><span class="line"># ce-name ： 在当前PE的当前VPN上指定CE名称</span><br><span class="line"># id ce-id : VPN内CE的ID值 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>为CE创建连接</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">connection [ ce-offset id ] interface interface-type interface-number [ tunnel-policy policy-name ] [ raw | tagged ]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ce-offset id：与L2VPN相连的对端CE的ID。</span><br><span class="line"># interface interface-type interface-number ： 与CE相连的接口，其封装格式必须与所属VPN一致。</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h4 id="Kompella-1"><a href="#Kompella-1" class="headerlink" title="Kompella"></a>Kompella</h4><p>从图中可以看到，CE1与 CE3和CE13 都是点到点的连接，PE1会分配两个标签块，一个是用来与CE1建立连接的，一个是用来与CE13建立连接的，同样，PE2也给CE3分配了标签块，PE3给CE13分配了标签块<br><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gimg08533gj30z40iuwoh.jpg" alt="捕获.PNG"></p>
<h1 id="VPLS"><a href="#VPLS" class="headerlink" title="VPLS"></a>VPLS</h1><h2 id="VPLS基本结构"><a href="#VPLS基本结构" class="headerlink" title="VPLS基本结构"></a>VPLS基本结构</h2><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gi9q1b3vzmj314e0j7k38.jpg" alt="捕获.PNG"></p>
<ul>
<li>PW（Pseudo-Wire）：伪线。PW是VPLS中两个PE间的虚拟逻辑连接，在两个PE之间传输用户数据。</li>
<li>VSI（Virtual Switch Instance）：虚拟交换实例。每个VSI提供单独的VPLS服务。通过VSI，可以将VPLS的实际接入链路映射到各条虚链接上。</li>
<li>AC（Attachment Circuit）：接入线路。指CE与PE的连接，在L2VPN中，CE通过AC接入到PE。</li>
<li>Tunnel（Network Tunnel）：隧道。用于承载PW，一条隧道上可以承载多条PW，一般情况下为MPLS隧道。隧道是一条本地PE与对端PE之间的直连通道，完成PE之间的数据透传。</li>
</ul>
<h2 id="VPLS基本工作原理"><a href="#VPLS基本工作原理" class="headerlink" title="VPLS基本工作原理"></a>VPLS基本工作原理</h2><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gi9sctbxn7j312p0fyjxu.jpg" alt="捕获.PNG"></p>
<p>可以将每个VSI看作一台独立的交换机。PE连 接CE的端口（AC接口）作为交换机的端口，而连接各个PE的PW就是交换机的内部交叉电路。</p>
<h2 id="VPLS与普通L2VPN的区别"><a href="#VPLS与普通L2VPN的区别" class="headerlink" title="VPLS与普通L2VPN的区别"></a>VPLS与普通L2VPN的区别</h2><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gi9sfozbwpj30yq0jzwmz.jpg" alt="捕获.PNG"></p>
<h2 id="VPLS基本拓扑"><a href="#VPLS基本拓扑" class="headerlink" title="VPLS基本拓扑"></a>VPLS基本拓扑</h2><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gi9sk3h7khj30tj0d8n48.jpg" alt="捕获.PNG"></p>
<p>同一个VSI中各个PE间必须建立<strong>全连接</strong>的PW，同时配置“水平分割”策略（无需配置，产品缺省），即如果PE收到远端PE发来的广播流量，它只向同一VPLS的所有AC端口转发流量，不向其他PE转发 。PE间全连接和水平分割一起保证了VPLS转发的可达性和无环路。</p>
<h2 id="VPLS控制平面"><a href="#VPLS控制平面" class="headerlink" title="VPLS控制平面"></a>VPLS控制平面</h2><p>华为产品支持使用LDP或BGP实现VPLS的控制平面的功能，分别称为Martini方式的VPLS和Kompella方式的VPLS。</p>
<h3 id="Martini方式的VPLS"><a href="#Martini方式的VPLS" class="headerlink" title="Martini方式的VPLS"></a>Martini方式的VPLS</h3><p>采用LDP作为信令，需要手工指定PE的各对等体，由于同一VPLS中各PE之间需要建立全连接，每当有新的PE加入时，所有相关PE上都修改配置，导致可扩展性较差。但是PW实际是点到点链路，使用LDP进行PW的建立、维护和拆除更为有效。</p>
<h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"># 创建一个名称为company1的Martini方式VPLS的VSI，配置当前VSI实例的ID为1。</span><br><span class="line">&lt;HUAWEI&gt; system-view</span><br><span class="line">[HUAWEI] mpls lsr-id 1.1.1.1 </span><br><span class="line">[HUAWEI] mpls</span><br><span class="line">[HUAWEI-mpls] quit</span><br><span class="line">[HUAWEI] mpls l2vpn</span><br><span class="line">[HUAWEI-l2vpn] quit</span><br><span class="line">[HUAWEI] vsi company1 static</span><br><span class="line">[HUAWEI-vsi-company1] pwsignal ldp</span><br><span class="line">[HUAWEI-vsi-company1-ldp] vsi-id 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 配置VSI对等体，建立与一个或多个对端PE的PW连接</span><br><span class="line"># peer peer-address [ negotiation-vc-id vc-id ] pw pw-name</span><br><span class="line"># negotiation-vc-id vc-id ： 虚电路的唯一标识，一般用于两端VSI ID不同但要求互通的情况，参数vc-id不能与本端其他VSI配置的VSI ID相同，到同一个Peer的negotiate-vc-id指定的VC ID也不能相同</span><br><span class="line"># pw pw-name ： PW的名称，用于标识该PW，PW名称要求在同一VSI下唯一，在不同VSI下PW名称可以相同。</span><br><span class="line"></span><br><span class="line">&lt;HUAWEI&gt; system-view</span><br><span class="line">[HUAWEI] vsi aa static</span><br><span class="line">[HUAWEI-vsi-aa] pwsignal ldp</span><br><span class="line">[HUAWEI-vsi-aa-ldp] vsi-id 1</span><br><span class="line">[HUAWEI-vsi-aa-ldp] peer 1.1.1.1</span><br><span class="line">[HUAWEI-vsi-aa-ldp] peer 1.1.1.1 pw pw1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># l2 binding命令用于将二层接口绑定到VSI实例</span><br><span class="line"># l2 binding vsi vsi-name [ access-port ]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gimhjbge83j30wi0jfjxa.jpg" alt="捕获.PNG"></p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gimhjqd5jcj30g90it0u8.jpg" alt="捕获.PNG"></p>
<h3 id="Kompella方式的VPLS"><a href="#Kompella方式的VPLS" class="headerlink" title="Kompella方式的VPLS"></a>Kompella方式的VPLS</h3><p>采用BGP作为信令，可以通过配置VPN Target实现VPLS成员的自动发现，增加PE或删除PE时，所需的额外操作很少，因而具有较好的扩展性。</p>
<h4 id="配置-1"><a href="#配置-1" class="headerlink" title="配置"></a>配置</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 创建一个名称为company2的Komeplla方式VPLS的VSI</span><br><span class="line"></span><br><span class="line">[HUAWEI] vsi company2 auto</span><br><span class="line">[HUAWEI-vsi-company2] pwsignal bgp</span><br><span class="line"></span><br><span class="line">route-distinguisher route-distinguisher 配置VSI的RD</span><br><span class="line">vpn-target vpn-target&amp;&lt;1-16&gt; [ both | export-extcommunity | import-extcommunity ] 配置VSI的VPN-Target</span><br><span class="line"></span><br><span class="line"># 配置Site连接</span><br><span class="line">site site-id [ range site-range ] [ default-offset &#123; 0 | 1 &#125; ] ]</span><br><span class="line"></span><br><span class="line"># 配置接口绑定VSI</span><br><span class="line">l2 binding vsi vsi-name</span><br></pre></td></tr></table></figure>

<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gimhw2iexjj30wo0h70vv.jpg" alt="捕获.PNG"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/04/QoS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="lowkeysp">
      <meta itemprop="description" content="lowkeysp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="lowkeysp' Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/08/04/QoS/" class="post-title-link" itemprop="url">QoS</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-08-04 09:20:54" itemprop="dateCreated datePublished" datetime="2020-08-04T09:20:54+08:00">2020-08-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-05-11 17:25:03" itemprop="dateModified" datetime="2023-05-11T17:25:03+08:00">2023-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E9%80%9A/" itemprop="url" rel="index"><span itemprop="name">数通</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <meta name="referrer" content="no-referrer" />

<h1 id="IP-QoS三种模型"><a href="#IP-QoS三种模型" class="headerlink" title="IP QoS三种模型"></a>IP QoS三种模型</h1><ul>
<li>Best-Effort模型：目前Internet的缺省服务模型，主要实现技术是先进先出队列（FIFO）</li>
<li>IntServ模型: 业务通过信令向网络申请特定的QoS服务，网络在流量参数描述的范围内，预留资源以承诺满足该需求</li>
<li>DiffServ模型：当网络出现拥塞时，根据业务的不同服务等级约定，有差别地进行流量控制和转发来解决拥塞问题</li>
</ul>
<h1 id="FIFO模型"><a href="#FIFO模型" class="headerlink" title="FIFO模型"></a>FIFO模型</h1><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ghej7q1xlkj315c0llwtp.jpg" alt="捕获.PNG"></p>
<h1 id="MQC配置模式"><a href="#MQC配置模式" class="headerlink" title="MQC配置模式"></a>MQC配置模式</h1><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ghejgv4ao9j30zz0lpne1.jpg" alt="捕获.PNG"></p>
<p>配置流程为：先配置class-map,再配置policy-map，再配置到service-map，然后再应用到端口即可</p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#先是分类，分等级</span><br><span class="line"></span><br><span class="line">class-map match-all ipp7</span><br><span class="line">   description for Diamond</span><br><span class="line">   match precedence 7</span><br><span class="line">end-class-map</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 然后对于每一个等级，进行带宽的分配，限速等配置</span><br><span class="line">policy-map pm5-8000K-in</span><br><span class="line">    class ipp7</span><br><span class="line">      police rate 1200000 bps burst 150000 bytes peak-burst 150000 bytes</span><br><span class="line">    </span><br><span class="line">      conform-action set mpls experimental imposition7</span><br><span class="line">      exceed-action drop</span><br><span class="line">   ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#对于总的带宽，进行一个配置</span><br><span class="line"></span><br><span class="line">policy-map pm5-8000K-in.p</span><br><span class="line">  class class-default</span><br><span class="line">    service-policy pm5-8000K-in</span><br><span class="line">    police rate 8000000 bps burst 1000000 bytes peak-burst 1000000 bytes</span><br><span class="line">    conform-action transmit</span><br><span class="line">    exceed-action drop</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#把该policy应用到端口中去</span><br><span class="line"></span><br><span class="line">interface GigabitEthernet0/0/0/3.142</span><br><span class="line">  description For ...</span><br><span class="line">  service-policy input pm5-8000K-in.p</span><br><span class="line">  service-policy output pm5-8000K-out.p</span><br><span class="line">  vrf VPN12432</span><br><span class="line">  ipv4 address 10.122.122.122 255.255.255.252</span><br><span class="line">  encapsulation dot1q 111</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="VPN业务等级"><a href="#VPN业务等级" class="headerlink" title="VPN业务等级"></a>VPN业务等级</h1><ul>
<li>IPP7 &#x3D; 钻石</li>
<li>IPP5 &#x3D; 白金</li>
<li>IPP3+6 &#x3D; 金</li>
<li>IPP2 &#x3D; 银</li>
<li>IPP1 &#x3D; 铜</li>
<li>IPP0 &#x3D; 其他</li>
</ul>
<h1 id="分类和标记（Classification-and-Marking）"><a href="#分类和标记（Classification-and-Marking）" class="headerlink" title="分类和标记（Classification and Marking）"></a>分类和标记（Classification and Marking）</h1><p>对不同的包进行分类，如果没有分类的，则视为是一样的</p>
<p>分类完成后，需要对不同的包进行标记，通常使用IP包的IP precedence ，MPLS的EXP标记位进行分类</p>
<h2 id="分类的配置"><a href="#分类的配置" class="headerlink" title="分类的配置"></a>分类的配置</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">class-map match-all ipp7</span><br><span class="line">   description for Diamond</span><br><span class="line">   match precedence 7  # 匹配的是IP包的procedence标志位，也就是根据用户的IP包进行分类</span><br><span class="line">end-class-map</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class-map match-all ipp5</span><br><span class="line">   description for Premium</span><br><span class="line">   match precedence 5</span><br><span class="line">end-class-map</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class-map match-all ipp3</span><br><span class="line">   description for Gold</span><br><span class="line">   match precedence 3</span><br><span class="line">end-class-map</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class-map match-all ipp2</span><br><span class="line">   description for Silver</span><br><span class="line">   match precedence 2</span><br><span class="line">end-class-map</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h2 id="标记配置"><a href="#标记配置" class="headerlink" title="标记配置"></a>标记配置</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">policy-map pm5-8000K-in</span><br><span class="line">  class ipp7</span><br><span class="line">    police rate 1200000 bps burst 150000 bytes peak-burst 150000 bytes</span><br><span class="line">    conform-action set mpls experimental importion 7  #在MPLS的EXP上进行标记，逻辑上也就是从客户收过来IP包，通过IP Precedence进行分类，分类位ipp7的，在MPLS包上进行打标，设置为7</span><br><span class="line">    exceed-action drop</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  class ipp5</span><br><span class="line">     police rate 1600000 bps burst 200000 bytes peak-burst 200000 bytes</span><br><span class="line">    conform-action set mpls experimental importion 5</span><br><span class="line">    exceed-action drop  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  class ipp3</span><br><span class="line">    set mpls experimental importion 3 </span><br><span class="line"></span><br><span class="line">  class ipp2</span><br><span class="line">    set mpls experimental importion 2</span><br><span class="line"></span><br><span class="line">  class class-default</span><br><span class="line">    set mpls experimental importion 1</span><br><span class="line"></span><br><span class="line">end-policy-map  </span><br></pre></td></tr></table></figure>

<h1 id="流量监管和整形"><a href="#流量监管和整形" class="headerlink" title="流量监管和整形"></a>流量监管和整形</h1><h2 id="流量监管"><a href="#流量监管" class="headerlink" title="流量监管"></a>流量监管</h2><p>流量监管是限制进入网络的流量与突发，为网络的稳定提供了基本的QoS功能。</p>
<p>流量监管TP（Traffic Policing）的典型应用是监督进入网络的某一流量的规格，把它限制在一个合理的范围之内，并对超出部分的流量进行“惩罚”，以保护网络资源和运营商的利益。</p>
<h2 id="流量整形"><a href="#流量整形" class="headerlink" title="流量整形"></a>流量整形</h2><p>流量整形则是限制流出网络的流量与突发，为网络的稳定提供了基本的QoS功能。</p>
<p>流量整形TS（Traffic Shaping）的典型作用是限制流出某一网络的某一连接的正常流量与突发流量，使这类报文以比较均匀的速度向外发送，是一种主动调整流量输出速率的措施。</p>
<h2 id="Police限速"><a href="#Police限速" class="headerlink" title="Police限速"></a>Police限速</h2><p>对于Policing，对于限制外的，就直接丢弃了</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ghels3mlqpj30jl0lsqd4.jpg" alt="捕获.PNG"></p>
<p>对于shaping，会进行一个平滑，对于超限的，会有一个缓存区先缓存着，缓存区满了才会进行丢弃</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ghelurqctsj30jq0mvn6p.jpg" alt="捕获.PNG"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">policy-map pm5-8000K-in</span><br><span class="line">  class ipp7</span><br><span class="line">    #使用的是police配置</span><br><span class="line">    #burst 允许突发一点，</span><br><span class="line">    police rate 1200000 bps burst 150000 bytes peak-burst 150000 bytes</span><br><span class="line">    conform-action set mpls experimental importion 7 </span><br><span class="line">    exceed-action drop</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  class ipp5</span><br><span class="line">     police rate 1600000 bps burst 200000 bytes peak-burst 200000 bytes</span><br><span class="line">    conform-action set mpls experimental importion 5</span><br><span class="line">    exceed-action drop  </span><br></pre></td></tr></table></figure>






<h2 id="Shape限速"><a href="#Shape限速" class="headerlink" title="Shape限速"></a>Shape限速</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">policy-map pm5-8000K-out.p</span><br><span class="line"></span><br><span class="line">  class class-default</span><br><span class="line">  service-policy pm5-8000K-out</span><br><span class="line">  shape average 8000000bps #shape限速</span><br><span class="line"></span><br><span class="line">end-policy-map</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="拥塞管理及避免"><a href="#拥塞管理及避免" class="headerlink" title="拥塞管理及避免"></a>拥塞管理及避免</h1><ul>
<li>网络拥塞时，保证不同的优先级的报文得到不同的QoS待遇</li>
<li>将不同优先级的报文入不同的队列，不同队列将得到不同的调度优先级，概率或带宽保证</li>
</ul>
<h2 id="PQ-Priority-Queuing"><a href="#PQ-Priority-Queuing" class="headerlink" title="PQ: Priority Queuing"></a>PQ: Priority Queuing</h2><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ghem6esxtjj311z0mc7hk.jpg" alt="捕获.PNG"></p>
<p>问题：如果Queue 1一直不空的话，那么Queue 2 和Queue 3就饿死了 根本转发不了，因此这种策略往往只用在最重要的业务才会设置这个，优先转发</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">policy-map pm5-8000K-out</span><br><span class="line">  class ipp7</span><br><span class="line">    #配置一个PQ,</span><br><span class="line">    priority level 1</span><br><span class="line">    police rate percent 15</span><br><span class="line">    conform-action transmit </span><br><span class="line">    exceed-action drop</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="CBWFQ配置"><a href="#CBWFQ配置" class="headerlink" title="CBWFQ配置"></a>CBWFQ配置</h2><ul>
<li>Priority：优先转发</li>
<li>Bandwidth：带宽报障 在拥塞时发生作用，具体的就是，即使发生了拥塞，仍有这些带宽报障，当然，最高也不能超过Police</li>
<li>Police：带宽限速</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">policy-map pm5-8000K-out</span><br><span class="line">  class ipp7 </span><br><span class="line">    priority level 1</span><br><span class="line">    police rate percent 15</span><br><span class="line">    conform-action transmit</span><br><span class="line">    exceed-action drop</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  class ipp5 </span><br><span class="line">    bandwidth percent 5</span><br><span class="line">    police rate percent 25</span><br><span class="line">    conform-action transmit</span><br><span class="line">    exceed-action drop</span><br><span class="line"></span><br><span class="line">    queue-limit 32 packets</span><br><span class="line"></span><br><span class="line">  class ipp3 </span><br><span class="line">    bandwidth percent 30</span><br><span class="line">    police rate percent 50</span><br><span class="line">    conform-action transmit</span><br><span class="line">    exceed-action drop</span><br><span class="line"></span><br><span class="line">    queue-limit 384 packetsmit</span><br><span class="line">    random-detect precedence 3 100 packets 280 packets</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  class ipp2 </span><br><span class="line">    bandwidth percent 25</span><br><span class="line">    police rate percent 75</span><br><span class="line">    conform-action transmit</span><br><span class="line">    exceed-action drop</span><br><span class="line"></span><br><span class="line">    queue-limit 384 packetsmit</span><br><span class="line">    random-detect precedence 2 80 packets 225 packets</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  class class-default </span><br><span class="line">    bandwidth percent 25</span><br><span class="line">    queue-limit 384 packetsmit</span><br><span class="line">    random-detect precedence 6,4,1,0,73 packets 150 packets</span><br><span class="line"></span><br><span class="line">end-policy-map</span><br></pre></td></tr></table></figure>

<h2 id="拥塞避免"><a href="#拥塞避免" class="headerlink" title="拥塞避免"></a>拥塞避免</h2><p>在网络没有发生拥塞以前根据队列状态进行有选择性的丢包</p>
<p>算法：</p>
<ul>
<li>RED  ： Random Early Detection</li>
<li>WRED ： Weighted Random Early Detection</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ghen0jj02xj31400nk12r.jpg" alt="捕获.PNG"></p>
<p>横坐标时队列的大小，在0-32的时候，是不丢包的，在32-40的时候，是随机丢，40之后，就全部丢，tail drop &#x3D;&#x3D; full drop</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ghenccq7rxj311q0g77gf.jpg" alt="捕获.PNG"></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 针对优先级为3的流量使用WRED策略</span><br><span class="line"># 100以内，不丢，100-280 随机丢 </span><br><span class="line">random-detect precedence 3 100 packets 280 packets</span><br></pre></td></tr></table></figure>

<h1 id="QoS相关命令"><a href="#QoS相关命令" class="headerlink" title="QoS相关命令"></a>QoS相关命令</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">show running class-maps</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show running policy-map</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show running-config interface </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show policy-map interface  &lt;interface&gt; &lt;input|output&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show qos interface &lt;interface&gt; &lt;input|output&gt;</span><br></pre></td></tr></table></figure>


<ul>
<li>policed and dropped ：如果超过限速的值，那么就丢弃了</li>
<li>RED random drops: 随机丢弃</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gher5eljt9j319h0olqju.jpg" alt="tempsnip.png"></p>
<h1 id="华为的QoS"><a href="#华为的QoS" class="headerlink" title="华为的QoS"></a>华为的QoS</h1><h2 id="In方向策略"><a href="#In方向策略" class="headerlink" title="In方向策略"></a>In方向策略</h2><p>通过上面思科系统的学习，知道了In方向通常只是对IP包进行分类（通过IP包的precedence），然后会有一个限速</p>
<h3 id="Classifier-分类"><a href="#Classifier-分类" class="headerlink" title="Classifier 分类"></a>Classifier 分类</h3><p>分类：根据IP包的precedence进行分类</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">traffic classifier dsipp7 operator or</span><br><span class="line">  if-match ip-precedence 7</span><br><span class="line"></span><br><span class="line">traffic classifier dsipp5 operator or</span><br><span class="line">  if-match ip-precedence 5</span><br><span class="line"></span><br><span class="line">traffic classifier dsipp3+6 operator or</span><br><span class="line">  if-match ip-precedence 3</span><br><span class="line">  if-match ip-precedence 6</span><br><span class="line"></span><br><span class="line">traffic classifier dsipp2 operator or</span><br><span class="line">  if-match ip-precedence 2</span><br><span class="line"></span><br><span class="line">traffic classifier others operator or</span><br><span class="line">  if-match ip-precedence 0</span><br><span class="line">  if-match ip-precedence 1</span><br><span class="line">  if-match ip-precedence 4 </span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="Behavior-动作"><a href="#Behavior-动作" class="headerlink" title="Behavior 动作"></a>Behavior 动作</h3><p>对不同类的一些行为操作，比如限速等</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"># cir表示指定承诺信息速率</span><br><span class="line"># cbs表示指定突发尺寸，即令牌桶的容量</span><br><span class="line"># pbs表示指定峰值突发尺寸</span><br><span class="line"></span><br><span class="line">traffic behavior dsipp7</span><br><span class="line">  car cir &lt;D&gt; cbs XXXX pbs XXXX</span><br><span class="line">  service-class cs7 color green</span><br><span class="line"></span><br><span class="line">traffic behavior dsipp5</span><br><span class="line">  car cir &lt;D+P&gt; cbs XXXX pbs XXXX</span><br><span class="line">  service-class ef color green</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">traffic behavior others</span><br><span class="line">  service-class af1 color green</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h3 id="policy"><a href="#policy" class="headerlink" title="policy"></a>policy</h3><p>将classifier和behavior合并之后，就变成了policy，也就是对于每一个分类，都有相应的行为操作</p>
<p>Qos-profile的user-queue配置是对整个端口的限速，behavior中的限速是对某一个ipp的限速，Behavior的限速必定 小于等于 user-queue的限速</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">traffic policy XXX-in</span><br><span class="line">  classifier dsipp7 behavior dsipp7 </span><br><span class="line">  classifier dsipp5 behavior dsipp5</span><br><span class="line">  classifier dsipp3+6 behavior dsipp3+6</span><br><span class="line">  classifier dsipp2 behavior dsipp2</span><br><span class="line">  classifier others behavior others</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># cir表示指定承诺信息速率</span><br><span class="line"># pir表示指定峰值速率</span><br><span class="line"></span><br><span class="line">qos-profile XXX-in.p</span><br><span class="line">  user-queue cir 2048 pir 2048</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h3 id="流量监管（CAR）"><a href="#流量监管（CAR）" class="headerlink" title="流量监管（CAR）"></a>流量监管（CAR）</h3><p>首先在接口下配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">interface Serial4/0/0/3:0</span><br><span class="line">   traffic-policy AAA-2M-in inbound</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>其中，AAA-2M-in为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">traffic policy AAA-2M-in</span><br><span class="line"> share-mode</span><br><span class="line"> classifier ipp7 behavior bh-ipp7-AAA-2M-in</span><br><span class="line"> classifier ipp5 behavior bh-ipp5-AAA-2M-in</span><br><span class="line"> classifier all behavior bh-ipp2-in</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>其中，bh-ipp7-AAA-2M-in</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">traffic behavior bh-ipp7-AAA-2M-in</span><br><span class="line"> car cir 304 cbs 57000 pbs 57000 green pass yellow pass red discard</span><br><span class="line"> service-class cs7 color green no-remark</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>其中，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">traffic classifier ipp7 operator or</span><br><span class="line"> if-match ip-precedence 7</span><br><span class="line"> if-match mpls-exp 7</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>配置检查命令</p>
<p>查看behavior</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display traffic behavior user-defined bh-ipp7-AAA-2M-in</span><br></pre></td></tr></table></figure>
<p>查看traffic policy</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display traffic policy user-defined AAA-2M-in</span><br></pre></td></tr></table></figure>
<p>查看classifier</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display traffic classifier user-defined ipp7</span><br></pre></td></tr></table></figure>


<h3 id="流量整形（flow-queue方式）"><a href="#流量整形（flow-queue方式）" class="headerlink" title="流量整形（flow-queue方式）"></a>流量整形（flow-queue方式）</h3><p>queue命令用来在流队列模板中修改流队列的调度参数，包括：调度方式、shaping、WRED。</p>
<p>参数：</p>
<ul>
<li>cos-value，指定配置的流队列，取值可以是af1～af4、be、cs6、cs7、ef</li>
<li>weight-value，整数形式，取值范围是1～100</li>
<li>shaping-value，整形速率，表示每个流队列的整形速率，整数形式，取值范围是8～4294967294，单位为Kbit&#x2F;s。</li>
<li>shaping-percentage-value，整形速率的百分比，表示整形带宽占Qos模板中用户队列峰值带宽的百分比。整数形式，取值范围是0～100</li>
<li>pq | wfq | lpq ： 配置该队列的调度方式。pq为绝对优先级队列调度；wfq为加权公平队列调度；lpq为低优先级调度。三种队列调度的优先级次序为：PQ队列的优先级高于WFQ队列的优先级。WFQ队列的优先级高于LPQ队列的优先级。高优先级的队列可以抢占低优先级队列的带宽。</li>
<li>pbs pbs-value，指定峰值突发尺寸（Peak Burst Size）</li>
<li>wred-name，配置该队列使用的WRED对象</li>
</ul>
<p>举例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在流队列WRED模板对象test中，配置af1队列的权重为50，整形速率为6Mbit/s。</span><br><span class="line"></span><br><span class="line">queue af1 wfq weight 50 shaping 6000 pbs 1000 flow-wred test</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="配置方法："><a href="#配置方法：" class="headerlink" title="配置方法："></a>配置方法：</h4><p>老的配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#老的配置使用的是user-queue,新的使用的是qos-profile，</span><br><span class="line">interface GigabitEthernet3/0/0.402</span><br><span class="line">  user-queue cir 18227 pir 18227 flow-queue AAA-20M-out outbound</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">flow-queue AAA-20M-out</span><br><span class="line"> queue af2 wfq weight 34 flow-wred ipp2-wred</span><br><span class="line"> queue af3 wfq weight 66 flow-wred ipp3-wred</span><br><span class="line"> queue cs7 pq shaping shaping-percentage 30</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>新的配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">interface GigabitEthernet2/1/0.153</span><br><span class="line"></span><br><span class="line"> qos-profile AAA-6M-out.p outbound identifier none</span><br><span class="line"> statistic enable</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">qos-profile AAA-6M-out.p</span><br><span class="line"> user-queue cir 6144 pir 6144 flow-queue AAA-6M-out outbound</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">flow-queue AAA-6M-out</span><br><span class="line"> queue af2 wfq weight 24 shaping shaping-percentage 100 flow-wred ipp2-wred</span><br><span class="line"> queue af3 wfq weight 46 shaping shaping-percentage 76 flow-wred ipp3-wred</span><br><span class="line"> queue ef wfq weight 30 shaping shaping-percentage 30 flow-wred ipp5-wred</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>查看配置命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display flow-queue configuration verbose AAA-20M-out</span><br></pre></td></tr></table></figure>



<h3 id="拥塞管理"><a href="#拥塞管理" class="headerlink" title="拥塞管理"></a>拥塞管理</h3><p>拥塞管理指网络在发生拥塞时，如何进行管理和控制。处理的方法是使用队列技术，将从一个接口发出的所有报文放入多个队列，按照各个队列的优先级进行处理。不同的队列调度算法用来解决不同的问题，并产生不同的效果。</p>
<p>比如，PQ队列，WFQ队列等等</p>
<h3 id="拥塞避免-1"><a href="#拥塞避免-1" class="headerlink" title="拥塞避免"></a>拥塞避免</h3><p>通过监视网络资源（如队列或内存缓冲区）的使用情况，在拥塞有加剧的趋势时，主动丢弃报文，通过调整网络的流量来解除网络过载的一种流量控制机制。拥塞避免用于防止因为线路拥塞而使设备的队列溢出。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">interface GigabitEthernet6/0/2.100        </span><br><span class="line"> </span><br><span class="line"> qos-profile AAA-out.p outbound identifier none</span><br><span class="line"> statistic enable</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">qos-profile AAA-out.p</span><br><span class="line"> user-queue cir 2000000 pir 2000000 flow-queue AAA-out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">flow-queue AAA-out</span><br><span class="line"> queue cs7 pq flow-wred ipp3-wred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">flow-wred ipp3-wred                       </span><br><span class="line"> color green low-limit 50 high-limit 100 discard-percentage 100</span><br><span class="line"></span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/07/09/MPLSVPN-%E7%BA%A2%E8%8C%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="lowkeysp">
      <meta itemprop="description" content="lowkeysp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="lowkeysp' Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/09/MPLSVPN-%E7%BA%A2%E8%8C%B6/" class="post-title-link" itemprop="url">MPLSVPN-红茶</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-09 20:29:24" itemprop="dateCreated datePublished" datetime="2020-07-09T20:29:24+08:00">2020-07-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-05-11 17:25:03" itemprop="dateModified" datetime="2023-05-11T17:25:03+08:00">2023-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">数据</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <meta name="referrer" content="no-referrer" />


<h1 id="MPLS-VPN"><a href="#MPLS-VPN" class="headerlink" title="MPLS VPN"></a>MPLS VPN</h1><p>优势：</p>
<ul>
<li>PE路由器与CE路由器运行动态路由协议</li>
<li>PE为每个客户维护一个独立的路由表</li>
<li>允许客户使用重叠的地址空间</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggxpof8g7xj30i40byjxg.jpg" alt="捕获.PNG"></p>
<h1 id="MPLS-初体验"><a href="#MPLS-初体验" class="headerlink" title="MPLS 初体验"></a>MPLS 初体验</h1><h2 id="路由层面"><a href="#路由层面" class="headerlink" title="路由层面"></a>路由层面</h2><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggybkm1rc2j310u0eo7k9.jpg" alt="捕获.PNG"></p>
<ul>
<li>PE会创建两个VRF，用于区分两个不同的客户，同时用于解决从客户上收过来的路由冲突</li>
<li>两个PE之间建立MP-BGP，用于传递vpnv4的路由表（vpnv4路由：RD值：ip地址&#x2F;掩码，比如：4813：12：10.1.1.0&#x2F;24）。RD值用来防止地址冲突。</li>
<li>另一端的PE路由器使用RT值来区分这两个路由，RT值可实际上是扩展的community，默认另一端的PE路由器的VRF都是拒收任何路由的，配上import的RT值之后，则可以接受该RT值的vpnv4的路由。然后将接受的vpnv4路由再转给相应的客户。</li>
</ul>
<p>可以看出，RD值和RT值是用来转发路由的，是路由层面的，和数据转发层面没有关系。</p>
<h2 id="数据层面"><a href="#数据层面" class="headerlink" title="数据层面"></a>数据层面</h2><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggybp11u3jj311q0f2dsf.jpg" alt="捕获.PNG"></p>
<ul>
<li>100的标签是左边的PE设备给右边通告的，用于区分左边的不同客户，当一个带有100标签的数据包传给左边的PE设备后，左边的PE设备就知道这个数据应该是给上面的黄色客户的。这个标签值也是通过MP-BGP协议传递给右边的PE设备的，和RT值传递的方式一样。这是内层的标签，可以说，内层的标签用来区分客户</li>
<li>301的标签是通过MPLS协议，P设备给右边的PE设备的，也就是建立双向的LDP。外层的标签用于在骨干网的转发。</li>
</ul>
<p>可以看出，内层标签是通过MP-BGP传递，外层标签则是MPLS，通过两层标签来进行数据层面的转发。</p>
<p>顶层标签有PHP机制，但是VPN标签不能再倒数第二跳弹出，因为Exgress PE需要这层标签</p>
<p>外层标签的数据转发是这样的，当PE2给PE1传递MP-BGP路由时，next-hop会是自身，然后，客户给PE1传递数据包后，PE1查找下一跳为PE2，然后发现使用的是MPLS，则会加上外层标签进行传递，而不是基于IP地址进行传递。</p>
<h1 id="MPLS-VPN架构-PE-router"><a href="#MPLS-VPN架构-PE-router" class="headerlink" title="MPLS VPN架构 -PE router"></a>MPLS VPN架构 -PE router</h1><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggybxxatlaj310m0ldtq9.jpg" alt="捕获.PNG"></p>
<p>一个PE设备会维护一个全局路由表和多个VRF路由表。</p>
<h2 id="RD值"><a href="#RD值" class="headerlink" title="RD值"></a>RD值</h2><p>用来区分冲突的私网路由，64bit</p>
<ul>
<li>用于在MP-BGP运载VRF的IPv4路由前缀时，确保这些前缀的唯一性</li>
<li>RD并不会说明该前缀属于哪一个VRF（需要搭配RT）</li>
<li>64bits的RD与32bit的IPv4前缀构成96bit的VPNv4前缀</li>
<li>如果不同的VPN客户，存在相同的IPv4地址空间，那么可以通过设置不同的RD值从而保障前缀的唯一性</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggycqgnotzj310j0juqit.jpg" alt="捕获.PNG"></p>
<h2 id="RT值"><a href="#RT值" class="headerlink" title="RT值"></a>RT值</h2><ul>
<li>RT值实际上就是MP-BGP的扩展的community</li>
<li>用于区分VPN（Customer）</li>
<li>一条路由可以附加多个RT值</li>
<li>import和export</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggycse0c3lj311l0gl12m.jpg" alt="捕获.PNG"></p>
<h1 id="VRF-MODEL"><a href="#VRF-MODEL" class="headerlink" title="VRF MODEL"></a>VRF MODEL</h1><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggkzk3a036j311l0k4k29.jpg" alt="捕获.PNG"></p>
<ul>
<li>一个VRF需要定义RD值和RT值（export和import）。默认不配import的话，是拒收任何路由的。</li>
<li>一个VRF可以有多个接口，一个接口只能属于一个VRF</li>
</ul>
<p>一个VRF可以想象成是一个虚拟路由器，有路由表，转发表，属于该VRF的接口，还有与VRF关联的其他信息，如RD值和RT值</p>
<h2 id="MP-BGP"><a href="#MP-BGP" class="headerlink" title="MP-BGP"></a>MP-BGP</h2><ul>
<li>BGP 能够承载大批量的路由前缀</li>
<li>BGP 拥有丰富的路径属性及丰富的策略部署工具</li>
<li>BGP 能够在非直连的Peer之间交互路由信息，因此P路由器无需运行BGP即无需维护客户的路由信息，大大减轻了负担</li>
</ul>
<p>MP-BGP在MPLS VPN中用于在PE路由器之间交互VPNv4路由， 同时为VPNv4路由分配标签</p>
<p>IPv4路由重发布进MP-BGP（如果PE-CE之间跑的就是BGP，则不用重发布路由）</p>
<p>一个MP-BGP的update报文所包含的元素有：</p>
<ul>
<li>VPNv4地址</li>
<li>扩展的community属性，如RT值，SOO等</li>
<li>用于VPN报文转发的标签</li>
<li>其他的普通的BGP的属性，如MED等等</li>
</ul>
<h1 id="路由汇总对MPLS-VPN数据传输的影响"><a href="#路由汇总对MPLS-VPN数据传输的影响" class="headerlink" title="路由汇总对MPLS VPN数据传输的影响"></a>路由汇总对MPLS VPN数据传输的影响</h1><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggydpep9qnj311q0n8aop.jpg" alt="捕获.PNG"></p>
<p>如果在P路由器上对PE的loopback地址做了汇总，由于汇总路由是在这台P设备上产生的，因此PHP机制在这个P设备上起效，在P设备上就进行了标签弹出，由于这台P设备面对的是一个VPN标签，但是这个P设备不认识这个VPN标签，那么就会丢弃。</p>
<h1 id="VRF-和-MP-BGP的配置"><a href="#VRF-和-MP-BGP的配置" class="headerlink" title="VRF 和 MP-BGP的配置"></a>VRF 和 MP-BGP的配置</h1><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggl09m806bj31070k3gym.jpg" alt="捕获.PNG"></p>
<ul>
<li><p>在PE上创建VRF，将PE-CE的接口放入VRF</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">ip vrf cisco</span><br><span class="line"> rd 1:1</span><br><span class="line"> route-target export 234:2       # 本地的RT export</span><br><span class="line"> route-target import 234:4       # 匹配PE2所配置的RT export</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">interface XX</span><br><span class="line"> ip vrf forwarding cisco         #将该接口放入VRF cisco</span><br><span class="line"> ip address XXXX XXXX</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"># 配置完之后，这个接口就不属于全局路由表中了，不会在全局路由表中出现这个接口，而是在VRF的虚拟路由表中，可以通过</span><br><span class="line">show ip route vrf cisco </span><br><span class="line">中查看vrf的虚拟路由表</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>PE配置MP-MGP</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[config] router bgp 234</span><br><span class="line">[config-router] bgp router-id 2.2.2.2</span><br><span class="line">[config-router] no bgp default ipv4-unicast # 如果我们不需要PE1与PE2之间交互IPv4前缀的话，可以将默认建立Ipv4的BGP邻居开关关掉</span><br><span class="line">[config-route] neighbor 4.4.4.4 remote 234</span><br><span class="line">[config-router] neighbor 4.4.4.4 update-source loopback0</span><br><span class="line">[config-router] address-family vpnv4</span><br><span class="line">[config-router-af] neighbor 4.4.4.4 activate</span><br><span class="line">[config-router] neighbor 4.4.4.4 send-community extended</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#查看MP-BGP的邻居</span><br><span class="line">[cisco] show ip bgp vpnv4 all summary</span><br></pre></td></tr></table></figure>

</li>
<li><p>完成PE-CE之间路由的重发布</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[config] router bgp 234</span><br><span class="line">[config-router] address-family ipv4 vrf cisco #必须在这个cisco中进行重发布</span><br><span class="line">[config-router-af] redistribute ospf 1 vrf cisco match internal external </span><br><span class="line"># internal表示只重发布ospf的内部路由，external表示只重发布ospf的外部路由</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">PE给CE的重发布</span><br><span class="line">[config] router ospf 1 vrf cisco</span><br><span class="line">[config-router] redistribute bgp 234 subnets</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>查看vpnv4的路由</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">show ip bgp vpnv4 all</span><br><span class="line"></span><br><span class="line">show ip bgp vpnv4 all X.X.X.X #查看某一条vpnv4路由</span><br></pre></td></tr></table></figure></li>
<li><p>查看本地给路由分配的标签,也就是内部标签</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show ip bgp vpnv4 all labels</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看vrf路由表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">show ip route vrf cisco</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>查看本地配置的vrf以及其下关联的接口</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show ip vrf</span><br></pre></td></tr></table></figure></li>
<li><p>查看vrf详细信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show ip vrf detail</span><br></pre></td></tr></table></figure></li>
<li><p>查看vrf接口</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show ip vrf interface</span><br></pre></td></tr></table></figure></li>
<li><p>查看VRF的CEF表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show ip cef vrf</span><br></pre></td></tr></table></figure></li>
<li><p>查看VRF CEF表的详细信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show ip cef vrf detail</span><br></pre></td></tr></table></figure></li>
<li><p>ping</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ping vrf vrfname dst-addr</span><br></pre></td></tr></table></figure></li>
<li><p>traceroute</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">traceroute vrf vrfname ? </span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="RR"><a href="#RR" class="headerlink" title="RR"></a>RR</h1><ul>
<li><p>在MPLS VPN网络中，RR和其他BGP设备（PE）的工作方式有所不同，在RT没有在RR配置的时候，RR并不会拒绝VPNv4路由，这点和PE不一样，PE路由器如果收到一条没有在任何RT输入到VRF的VPNv4路由的话，该路由就会被拒绝，PE采用这种方式来节省内存</p>
</li>
<li><p>没有必要让一个RR或一组RR拥有或反射BGP表里所有的VPNv4路由。可以将这些VPNv4路由分成几组，然后让多个RR或多组RR分别承载这几组路由。操作的方式是，在VPNv4地址簇模式中通过命令bgp rr-group extcommunity-list来实现，这个扩展的community-list用来指定希望通过这个RR允许或拒绝RT。</p>
</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggykexytyyj312j0kwqfv.jpg" alt="捕获.PNG"> </p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggykgbgn60j30z60nuguv.jpg" alt="捕获.PNG"></p>
<h1 id="PE与CE之间的路由协议"><a href="#PE与CE之间的路由协议" class="headerlink" title="PE与CE之间的路由协议"></a>PE与CE之间的路由协议</h1><h2 id="EBGP"><a href="#EBGP" class="headerlink" title="EBGP"></a>EBGP</h2><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggyl4g5ntij31180nr4cj.jpg" alt="捕获.PNG"></p>
<p>那么PE从CE学到的路由，直接就是BGP的路由条目，这个VRF-A routing table就是BGP路由表，那么，只需要加上RD前缀，RT值，标签等就能直接变成vpnv4路由传给对端PE，但是如果PE与CE是OSPF连接的话，那么学到的是OSPF路由条目，VRF-A routing table也是OSPF路由表，那么就需要重发布到MP-BGP中。</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggyljnpanxj311v0o4k6s.jpg" alt="捕获.PNG"></p>
<h2 id="静态路由"><a href="#静态路由" class="headerlink" title="静态路由"></a>静态路由</h2><p>在PE上做配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 需要在vrf中做</span><br><span class="line">ip route vrf cisco 1.1.1.1 255.255.255.255 10.1.12.1 e0/0</span><br><span class="line"></span><br><span class="line"># 重发布</span><br><span class="line">router bgp 234</span><br><span class="line">  address-family ipv4 vrf cisco</span><br><span class="line">    redistribute static</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggyl6uj7zuj31140ogn96.jpg" alt="捕获.PNG"></p>
<h1 id="SOO"><a href="#SOO" class="headerlink" title="SOO"></a>SOO</h1><p>使用SOO用于防环路,Site of Origin</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggym74mybcj311b0foak6.jpg" alt="捕获.PNG"></p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggymaqh4tnj310j0hu4d5.jpg" alt="捕获.PNG"></p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggymbd64imj310u0fxtnd.jpg" alt="捕获.PNG"></p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggymc688ubj310u0hcwtk.jpg" alt="捕获.PNG"></p>
<p>解决方法：</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggymfglmsdj311e0jltru.jpg" alt="捕获.PNG"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/07/08/MPLS-VPN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="lowkeysp">
      <meta itemprop="description" content="lowkeysp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="lowkeysp' Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/08/MPLS-VPN/" class="post-title-link" itemprop="url">MPLS VPN</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-08 11:47:19" itemprop="dateCreated datePublished" datetime="2020-07-08T11:47:19+08:00">2020-07-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-05-11 17:25:03" itemprop="dateModified" datetime="2023-05-11T17:25:03+08:00">2023-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MPLS/" itemprop="url" rel="index"><span itemprop="name">MPLS</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <meta name="referrer" content="no-referrer" />

<ul>
<li>CE（Customer Edge）设备：企业用户的网络设备</li>
<li>PE（Provider Edge）设备：运营商提供边缘设备</li>
<li>P（Provider）设备：运营商骨干设备</li>
</ul>
<h1 id="MPLS-VPN产生的原因"><a href="#MPLS-VPN产生的原因" class="headerlink" title="MPLS VPN产生的原因"></a>MPLS VPN产生的原因</h1><p>两个客户的VPN存在相同的地址空间（私网地址），传统VPN网络结构中的设备无法区分客户重叠的路由信息</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggjjllqe9ij31rk0tk7wh.jpg" alt="捕获.PNG"></p>
<p>解决VPN客户地址空间重叠问题需要解决上述三个问题。</p>
<p> 解决方案：</p>
<p>解决问题1：</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggjjxhvogej31o10jsnmc.jpg" alt="捕获.PNG"></p>
<p> 在共享PE设备上使用VRF（VPN Routing and Forwarding VPN路由转发）技术将重叠的路由隔离：每个VPN的路由放入自己对应的VPN Routing Table中</p>
<p> PE设备在维护多个VPN Routing Table时，同时还维护一个公网的路由表</p>
<p> PE设备除有多个不同实例的VPN路由表之外，还存在公网路由表（全局路由表）；每个PE设备对应一个或多个VPN实例，与PE设备接口进行一一绑定，每个VRF都需要一个VPN Instance，PE设备多个不同VPN实例相互隔离</p>
<p>解决问题3：</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggjk5rh1esj31p00w1hdt.jpg" alt="捕获.PNG"></p>
<p>将VPN路由发布到全局路由表之前，使用一个全局唯一的标识和路由绑定，以区分冲突的私网路由。这个标识被称为RD（Route Distinguisher）</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggjken5op4j31p60s47wh.jpg" alt="捕获.PNG"></p>
<p>但是这种还是存在一定的问题，比如两个分部和一个总部，这个RD到底是应该用1：1还是2：2，似乎都不行。</p>
<p>因此，RD并不能完美解决这一问题，因此又引入了RT来解决</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggjkio84bbj31l70sp4qp.jpg" alt="捕获.PNG"></p>
<p>RT属性用于将路由正确引入VPN，有两类VPN Target属性，Import Target和Export Target，分别用于VPN路由的导出与导入。</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggjkydnv78j31su0wfhdt.jpg" alt="捕获.PNG"></p>
<p>使用RT实现本端与对端的路由正确引入VPN，原则如下：</p>
<ul>
<li>本端的Export Target &#x3D; 对端的Import Target，本端的Import Target &#x3D; 对端的Export Target</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggjl3aglmaj31wz0ze7wh.jpg" alt="捕获.PNG"></p>
<p>因为数据包没有携带任何标识，所以在ICMP的数据包到达PE1时，PE1并不知道该查找哪个VPN的路由表找到正确的目的地址。</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggjl8cueetj31z5167u0x.jpg" alt="捕获.PNG"></p>
<p>使用标签嵌套解决数据转发过程中冲突路由的查找问题</p>
<h1 id="MPLS-VPN的工作过程"><a href="#MPLS-VPN的工作过程" class="headerlink" title="MPLS VPN的工作过程"></a>MPLS VPN的工作过程</h1><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggjlic7kumj31vl0y24qp.jpg" alt="捕获.PNG"></p>
<p>MPLS VPN的工作过程分为两部分：</p>
<ul>
<li>MPLS VPN路由的传递过程</li>
<li>MPLS VPN数据的转发过程</li>
</ul>
<h2 id="CE与PE之间的路由交换"><a href="#CE与PE之间的路由交换" class="headerlink" title="CE与PE之间的路由交换"></a>CE与PE之间的路由交换</h2><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggjlkm2bdfj31ur0y77wh.jpg" alt="捕获.PNG"></p>
<p>PE与CE之间可以通过静态路由协议交换路由信息，也可以通过动态路由协议交换路由信息</p>
<h2 id="VPN路由注入MP-BGP-多协议BGP-的过程"><a href="#VPN路由注入MP-BGP-多协议BGP-的过程" class="headerlink" title="VPN路由注入MP-BGP(多协议BGP)的过程"></a>VPN路由注入MP-BGP(多协议BGP)的过程</h2><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggjlmvg1hdj31qp0s8b29.jpg" alt="捕获.PNG"></p>
<p>VRF中的IPv4路由被添加上RD，RT与标签等信息称为VPN-IPv4的路由放入到MP-BGP的路由表中，并通过MP-BGP协议在PE设备之间交换路由信息</p>
<p>MP-BGP是多协议BGP，可以用来传递VPN-IPv4路由，而普通的BGP协议是不能传递VPN-IPv4路由的</p>
<p>PE1设备与P设备之间是通过MPLS来转发的。</p>
<h2 id="公网标签的分配过程"><a href="#公网标签的分配过程" class="headerlink" title="公网标签的分配过程"></a>公网标签的分配过程</h2><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggjlykjlnsj31z20web29.jpg" alt="捕获.PNG"></p>
<ul>
<li>MPLS协议在运营商网络分配公网标签，建立标签隧道，实现私网数据在公网上的转发</li>
<li>PE之间运行的MP-BGP协议为VPN路由分配私网标签，PE设备根据私网标签将数据正确转发给相应的VPN</li>
</ul>
<h2 id="PE设备到CE设备的数据转发"><a href="#PE设备到CE设备的数据转发" class="headerlink" title="PE设备到CE设备的数据转发"></a>PE设备到CE设备的数据转发</h2><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggjm207ndej31un12ie81.jpg" alt="捕获.PNG"></p>
<p>PE1收到剥离公网标签的数据包后，根据私网标签查找转发数据包的下一跳，将数据包正确发给相应的VPN客户</p>
<h1 id="MPLS-VPN配置"><a href="#MPLS-VPN配置" class="headerlink" title="MPLS VPN配置"></a>MPLS VPN配置</h1><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggjmqk5b7yj325t1b0npg.jpg" alt="捕获.PNG"></p>
<p>关键配置：</p>
<p>PE1上面的配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#创建VPN实例</span><br><span class="line">[PE1] ip vpn-instance vpn1</span><br><span class="line">#使用ipv地址簇</span><br><span class="line">[PE1-vpn-instance-vpn1] ipv4-family</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">[PE1] ip vpn-instance vpn2</span><br><span class="line">[PE1-vpn-instance-vpn2] ipv4-family</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 绑定IP地址,如果直接给端口绑定的话，由于地址一样，会发生冲突</span><br><span class="line">[PE1] interface GigabitEthernet0/0/1</span><br><span class="line">[PE1-GigabitEthernet0/0/1]ip binding vpn-instance vpn1</span><br><span class="line">[PE1-GigabitEthernet0/0/1] ip address 192.168.1.1 24</span><br><span class="line"></span><br><span class="line">[PE1] interface GigabitEthernet0/0/2</span><br><span class="line">[PE1-GigabitEthernet0/0/2]ip binding vpn-instance vpn2</span><br><span class="line">[PE1-GigabitEthernet0/0/2] ip address 192.168.1.1 24</span><br><span class="line"></span><br><span class="line"># 配置RD</span><br><span class="line">[PE1] ip vpn-instance vpn1</span><br><span class="line">[PE1-vpn-instance-vpn1] route-distinguisher RD值</span><br><span class="line">#配置RT</span><br><span class="line">[PE1-vpn-instance-vpn1] vpn-target RT值 export-extcommunity</span><br><span class="line">[PE1-vpn-instance-vpn1] vpn-target RT值 import-extcommunity</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>配置MP-BGP</p>
<p>在PE2上</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[PE2] bgp 300</span><br><span class="line">[PE2-BGP] peer 3.3.3.3 as 300</span><br><span class="line">[PE2-BGP] peer 3.3.3.3 connected loopback0</span><br><span class="line">[PE2-BGP] peer 3.3.3.3  next-hop-local</span><br><span class="line">#支持vpnv4簇</span><br><span class="line">[PE2-BGP] ipv4-family vpnv4</span><br><span class="line">[PE2-BGP-af-vpnv4] peer 3.3.3.3 enable</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在PE2上配置BGP与CE3所在的AS 400建立BGP邻居</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">CE3建立BGP邻居的命令和普通建立是一样的，PE2有些不同</span><br><span class="line"></span><br><span class="line">[PE2] bgp 300</span><br><span class="line">[PE2-BGP] ipv4-family vpn-instance vpn3</span><br><span class="line">[PE2-BGP-vpn3] peer 192.168.2.2 as-number 400</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>






<h1 id="VRF-MODEL"><a href="#VRF-MODEL" class="headerlink" title="VRF MODEL"></a>VRF MODEL</h1><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1ggkzk3a036j311l0k4k29.jpg" alt="捕获.PNG"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/07/03/pytorch%E5%AE%98%E6%96%B960min/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="lowkeysp">
      <meta itemprop="description" content="lowkeysp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="lowkeysp' Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/03/pytorch%E5%AE%98%E6%96%B960min/" class="post-title-link" itemprop="url">pytorch官方60min</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-03 00:58:32" itemprop="dateCreated datePublished" datetime="2020-07-03T00:58:32+08:00">2020-07-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-05-11 17:25:03" itemprop="dateModified" datetime="2023-05-11T17:25:03+08:00">2023-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <meta name="referrer" content="no-referrer" />
# 参考链接
http://pytorch123.com/SecondSection/what_is_pytorch/


<h1 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h1><p>Tensors 类似于 NumPy 的 ndarrays ，同时 Tensors 可以使用 GPU 进行计算。</p>
<p>比如，我们构造一个5*3的矩阵，不初始化</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">&gt;&gt;&gt; x = torch.empty(5,3)</span><br><span class="line">&gt;&gt;&gt; x</span><br><span class="line">tensor([[-1.2141e-25,  4.5783e-41, -1.2141e-25],</span><br><span class="line">        [ 4.5783e-41,  1.3563e-19,  1.3563e-19],</span><br><span class="line">        [ 1.3563e-19,  1.3563e-19,  1.3563e-19],</span><br><span class="line">        [ 1.3563e-19,  6.1678e+16,  6.4890e-07],</span><br><span class="line">        [ 2.6176e-12,  4.0058e-11,  2.5787e-09]])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>构造一个随机初始化的矩阵：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.rand(5,3)</span><br><span class="line">tensor([[0.0276, 0.6181, 0.1152],</span><br><span class="line">        [0.2565, 0.2421, 0.5149],</span><br><span class="line">        [0.9311, 0.2453, 0.3041],</span><br><span class="line">        [0.6361, 0.8637, 0.1596],</span><br><span class="line">        [0.7435, 0.8903, 0.9017]])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>构造一个矩阵为全0，而且数据类型是long</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.zeros(5,3,dtype=torch.long)</span><br><span class="line">tensor([[0, 0, 0],</span><br><span class="line">        [0, 0, 0],</span><br><span class="line">        [0, 0, 0],</span><br><span class="line">        [0, 0, 0],</span><br><span class="line">        [0, 0, 0]])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>构造一个张量，直接使用数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.tensor([5.5,3])</span><br><span class="line">tensor([5.5000, 3.0000])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>创建一个<strong>基于一个已知的tensor</strong>的tensor</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt;&gt;&gt; x = x.new_ones(5,3,dtype = torch.double)</span><br><span class="line">&gt;&gt;&gt; x</span><br><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]], dtype=torch.float64)</span><br><span class="line"></span><br><span class="line">#创建一个和x类似的y</span><br><span class="line">&gt;&gt;&gt; y = torch.randn_like(x,dtype=torch.float)</span><br><span class="line">&gt;&gt;&gt; y</span><br><span class="line">tensor([[ 0.0115,  0.2386,  0.6542],</span><br><span class="line">        [-0.1891, -0.6507,  1.0276],</span><br><span class="line">        [ 1.2823, -0.1774,  0.1024],</span><br><span class="line">        [-0.0743,  0.7157, -1.0440],</span><br><span class="line">        [-0.7562, -0.6935, -0.0515]])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>获得tensor的维度信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x.size()</span><br><span class="line">torch.Size([5, 3])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>加法操作</p>
<p>第一种：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x+y</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>第二种</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">torch.add(x,y)</span><br><span class="line"></span><br><span class="line">#使用下面方法，可以将结果付给result</span><br><span class="line">torch.add(x,y,out=result) </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>第三种</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y.add_(x)</span><br><span class="line"></span><br><span class="line"># 解释： 把x加到y上去，类似于 y = x + y</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>可以使用标准的Numpy类似的索引操作</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">x[:,1]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>如果你想改变一个tensor的大小或者形状，可以使用torch.view</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt;&gt;&gt; x = torch.randn(4,4)</span><br><span class="line">&gt;&gt;&gt; y = x.view(16)</span><br><span class="line">&gt;&gt;&gt; z = x.view(-1,8) #-1表示是另一个维度，即列向量</span><br><span class="line">&gt;&gt;&gt; x.size()</span><br><span class="line">torch.Size([4, 4])</span><br><span class="line">&gt;&gt;&gt; y.size()</span><br><span class="line">torch.Size([16])</span><br><span class="line">&gt;&gt;&gt; z.size()</span><br><span class="line">torch.Size([2, 8])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>如果你有一个元素tensor，可以使用.item()来获得这个value</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt;&gt;&gt; x = torch.randn(1)</span><br><span class="line">&gt;&gt;&gt; x</span><br><span class="line">tensor([-0.2879])</span><br><span class="line">&gt;&gt;&gt; x.item()</span><br><span class="line">-0.28786054253578186</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Torch Tensor 和 Numpy Array 是使用的同样的内存位置（前提是Torch Tensor 是在CPU上运行的），更改其中的一个也会影响到另一个</p>
<p>把Torch Tensor 转换成 Numpy Array</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;a = torch.ones(5)</span><br><span class="line">&gt;&gt;print(a)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([1.,1.,1.,1.,1.,])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt; b = a.numpy()</span><br><span class="line">&gt;&gt; print(b)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">[1. 1. 1. 1. 1.]</span><br></pre></td></tr></table></figure>
<p>让我们看看改变其中一个，另一个是否也会改变吧</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; a.add_(1)</span><br><span class="line">&gt;&gt; print(a)</span><br><span class="line">&gt;&gt; print(b)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([2., 2., 2., 2., 2.])</span><br><span class="line">[2. 2. 2. 2. 2.]</span><br></pre></td></tr></table></figure>
<p>看到没，我们只是改变了a，但是b也相应发生了改变，因为他们是共享同一块内存的</p>
<p>把Numpy Array转换成Torch Tensor</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">a = np.ones(5)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, 1, out=a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">[2. 2. 2. 2. 2.]</span><br><span class="line">tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</span><br></pre></td></tr></table></figure>
<p>也是，改变Numpy Array也会改变torch tensor</p>
<p>在CPU上，除了CharTensor之外，其他所有的Tensor都可以转换成Numpy Array</p>
<p>可以使用<code>.to</code>方法让Tensor支持任意设备</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># let us run this cell only if CUDA is available</span><br><span class="line"># We will use ``torch.device`` objects to move tensors in and out of GPU</span><br><span class="line">if torch.cuda.is_available():</span><br><span class="line">    device = torch.device(&quot;cuda&quot;)          # a CUDA device object</span><br><span class="line">    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU</span><br><span class="line">    x = x.to(device)                       # or just use strings ``.to(&quot;cuda&quot;)``</span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(&quot;cpu&quot;, torch.double))       # ``.to`` can also change dtype together!</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([0.6469], device=&#x27;cuda:0&#x27;)</span><br><span class="line">tensor([0.6469], dtype=torch.float64)</span><br></pre></td></tr></table></figure>
<p>可以看到，第一个tensor是cuda的，第二个就是正常的。</p>
<h1 id="AUTOGRAD-AUTOMATIC-DIFFERENTIATION"><a href="#AUTOGRAD-AUTOMATIC-DIFFERENTIATION" class="headerlink" title="AUTOGRAD: AUTOMATIC DIFFERENTIATION"></a>AUTOGRAD: AUTOMATIC DIFFERENTIATION</h1><p><code>autograd</code>这个包可以提供自动梯度。</p>
<p><code>torch.Tensor</code>是这个包的核心类，如果你设置Tensor的<code>.requires_grad</code>为<code>True</code>的话，那么它会开始追踪你的所有的运算，进而可以算梯度。当你完成计算时，只需要调用<code>.backward()</code>,就可以自动地计算出所有的梯度。这个tensor的梯度会被累加在这个<code>.grad</code>属性值里</p>
<p>使用<code>.detach()</code>可以停止追踪计算</p>
<p>还可以使用代码块<code>with torch.no_grad()</code>来停止追踪计算。这在评估模型的时候会非常有用，因为评估模型的时候，模型可能会含有可训练的参数，且<code>.requires_grad = True</code>，但是在评估模型的时候，并不需要梯度，因此可以使用这个代码块关闭追踪计算</p>
<p><code>Function</code>这个类在计算梯度的时候也是非常重要的。</p>
<p>每一个Tensor都有一个<code>.grad_fn</code>属性，用来指向创建这个Tensor的<code>Function</code>，除非这个Tensor在创建的时候，人为的设置<code>grad_fn</code>属性为<code>None</code>（下面的例子有解释）</p>
<p>举例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">x = torch.ones(2, 2, requires_grad=True)</span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([[1., 1.],</span><br><span class="line">        [1., 1.]], requires_grad=True)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>对这个x进行一次加法操作</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">y = x + 2</span><br><span class="line">print(y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line"></span><br><span class="line">tensor([[3., 3.],</span><br><span class="line">        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>这个y是通过加法运算得来的，因此他会有<code>grad_fn</code>属性</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(y.grad_fn)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">&lt;AddBackward0 object at 0x7f191afd60f0&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>对y再进行运算</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">z = y * y * 3</span><br><span class="line">out = z.mean()</span><br><span class="line"></span><br><span class="line">print(z, out)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([[27., 27.],</span><br><span class="line">        [27., 27.]], grad_fn=&lt;MulBackward0&gt;) tensor(27., grad_fn=&lt;MeanBackward0&gt;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><code>.requires_grad_( ... )</code>可以改变Tensor的<code>requires_grad</code>属性。如果不指定的话，默认是False的。下面这个例子，刚开始a的<code>requires_grad</code>属性默认是False的，后来通过这个函数，可以改成True。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(2, 2)</span><br><span class="line">a = ((a * 3) / (a - 1))</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">a.requires_grad_(True)</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">b = (a * a).sum()</span><br><span class="line">print(b.grad_fn)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out：</span><br><span class="line">False</span><br><span class="line">True</span><br><span class="line">&lt;SumBackward0 object at 0x7f191afd6be0&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>




<p>现在来计算Backprop,因为out是一个标量，所以<code>out.backward()</code>相当于是<code>out.backward(torch.tensor(1.))</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">out.backward()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>打印d(out)&#x2F;dx 的梯度</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(x.grad)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([[4.5000, 4.5000],</span><br><span class="line">        [4.5000, 4.5000]])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这个4.5的值是怎么得到的呢？</p>
<p>Tensor <code>out</code> 表示成$o$,我们有$o &#x3D; \frac{1}{4}\sum_i z_i$,$z_i &#x3D; 3(x_i+2)^2$,和$z_i\bigr\rvert_{x_i&#x3D;1} &#x3D; 27$。因此，有$\frac{\partial o}{\partial x_i} &#x3D; \frac{3}{2}(x_i+2)$,所以，$\frac{\partial o}{\partial x_i}\bigr\rvert_{x_i&#x3D;1} &#x3D; \frac{9}{2} &#x3D; 4.5$</p>
<p>在数学上，如果你有一个关于向量的函数，$\vec{y}&#x3D;f(\vec{x})$,那么关于$\vec{x}$的$\vec{y}$的梯度是一个Jacobian矩阵:</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gg8xlresm2j307f04gmx1.jpg" alt="捕获.PNG"></p>
<p>因此，可以说<code>torch.autograd</code>是用来计算vector-Jacobian product。也就是，给任意一个向量，$v&#x3D;\left(\begin{array}{cccc} v_{1} &amp; v_{2} &amp; \cdots &amp; v_{m}\end{array}\right)^{T}$,计算$v^{T}\cdot J$. 如果$v$刚好是函数$l&#x3D;g\left(\vec{y}\right)$的梯度，也就是$v&#x3D;\left(\begin{array}{ccc}\frac{\partial l}{\partial y_{1}} &amp; \cdots &amp; \frac{\partial l}{\partial y_{m}}\end{array}\right)^{T}$,那么根据chain rule，$v^{T}\cdot J$就是我们要求的$l$关于$\vec{x}$的梯度。（总结一下，就是如果要求梯度的话，如果我们有了Jacobian矩阵和向量$v$，那么我们就可以求出这个梯度）</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gg8xuj85rjj30bl04v3yi.jpg" alt="捕获.PNG"></p>
<p>我们来看一个例子</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(3, requires_grad=True)</span><br><span class="line"></span><br><span class="line">y = x * 2</span><br><span class="line">while y.data.norm() &lt; 1000:</span><br><span class="line">    y = y * 2</span><br><span class="line"></span><br><span class="line">print(y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([-970.9141,  465.0858, 1599.4425], grad_fn=&lt;MulBackward0&gt;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在这个例子里，$y$不是一个scalar，<code>torch.autograd</code>无法直接计算整个Jocobian，但是，如果我们只想要vector-Jacobian product的话，只需要把一个向量传给<code>backward</code>即可</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)</span><br><span class="line">y.backward(v)</span><br><span class="line"></span><br><span class="line">print(x.grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>当想要停止追踪计算时，如果Tensor的<code>requires_grad</code>属性是True时，可以使用<code>with torch.no_grad():</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">print(x.requires_grad)</span><br><span class="line">print((x ** 2).requires_grad)</span><br><span class="line"></span><br><span class="line">with torch.no_grad():</span><br><span class="line">    print((x ** 2).requires_grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">False</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>还可以使用<code>.detach()</code>得到一个新的Tensor，这个Tensor和之前的Tensor是一样的，只是没有了<code>requires_grad=True</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">print(x.requires_grad)</span><br><span class="line">y = x.detach()</span><br><span class="line">print(y.requires_grad)</span><br><span class="line">print(x.eq(y).all())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">True</span><br><span class="line">False</span><br><span class="line">tensor(True)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>参考一篇知乎讲解：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/65609544">https://zhuanlan.zhihu.com/p/65609544</a></p>
<h1 id="NEURAL-NETWORKS"><a href="#NEURAL-NETWORKS" class="headerlink" title="NEURAL NETWORKS"></a>NEURAL NETWORKS</h1><p>神经网络的搭建可以使用<code>torch.nn</code>包</p>
<p>一个<code>nn.Module</code>包括layers，一个<code>forward(input)</code>方法和一个<code>output</code></p>
<p>一个神经网络的训练过程如下：</p>
<ul>
<li>Define the neural network that has some learnable parameters (or weights)</li>
<li>Iterate over a dataset of inputs</li>
<li>Process input through the network</li>
<li>Compute the loss (how far is the output from being correct)</li>
<li>Propagate gradients back into the network’s parameters</li>
<li>Update the weights of the network, typically using a simple update rule: <code>weight = weight - learning_rate * gradient</code></li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gg8ztjlr3pj315f0cn77x.jpg" alt="捕获.PNG"></p>
<p>让我们定义这个神经网络：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Net(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        # 1 input image channel, 6 output channels, 3x3 square convolution</span><br><span class="line">        # kernel</span><br><span class="line">        self.conv1 = nn.Conv2d(1, 6, 3)</span><br><span class="line">        self.conv2 = nn.Conv2d(6, 16, 3)</span><br><span class="line">        # an affine operation: y = Wx + b</span><br><span class="line">        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension</span><br><span class="line">        self.fc2 = nn.Linear(120, 84)</span><br><span class="line">        self.fc3 = nn.Linear(84, 10)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # Max pooling over a (2, 2) window</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))</span><br><span class="line">        # If the size is a square you can only specify a single number</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), 2)</span><br><span class="line">        x = x.view(-1, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">    def num_flat_features(self, x):</span><br><span class="line">        size = x.size()[1:]  # all dimensions except the batch dimension</span><br><span class="line">        num_features = 1</span><br><span class="line">        for s in size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        return num_features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line"></span><br><span class="line">Net(</span><br><span class="line">  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))</span><br><span class="line">  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))</span><br><span class="line">  (fc1): Linear(in_features=576, out_features=120, bias=True)</span><br><span class="line">  (fc2): Linear(in_features=120, out_features=84, bias=True)</span><br><span class="line">  (fc3): Linear(in_features=84, out_features=10, bias=True)</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>你只需要定义<code>forward</code>函数,  <code>backward</code>函数会自动定义的（使用<code>autograd</code>）。你可以在<code>forward</code>函数里使用任意Tensor的operation</p>
<p>模型里面的可学习的参数可以通过<code>net.parameters()</code>学到</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">params = list(net.parameters())</span><br><span class="line">print(len(params))</span><br><span class="line">print(params[0].size())  # conv1&#x27;s .weight</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">10</span><br><span class="line">torch.Size([6, 1, 3, 3])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>让我们试一下32*32的input。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">input = torch.randn(1, 1, 32, 32)</span><br><span class="line">out = net(input)</span><br><span class="line">print(out)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([[ 0.1242,  0.1194, -0.0584, -0.1140,  0.0661,  0.0191, -0.0966,  0.0480,</span><br><span class="line">          0.0775, -0.0451]], grad_fn=&lt;AddmmBackward&gt;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>之后清零 所有参数的梯度缓存 然后 使用backprops</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(1, 10))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>NOTE:<blockquote>
<p>torch.nn only supports mini-batches. The entire torch.nn package only supports inputs that are a mini-batch of samples, and not a single sample.</p>
<p>For example, nn.Conv2d will take in a 4D Tensor of nSamples x nChannels x Height x Width.</p>
<p>If you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension.</p>
</blockquote>
</li>
</ul>
<p>回顾一下学到的：</p>
<ul>
<li><code>torch.Tensor</code> - A multi-dimensional array with support for autograd operations like <code>backward()</code>. Also holds the gradient w.r.t. the tensor.</li>
<li><code>nn.Module</code> - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.</li>
<li><code>nn.Parameter</code> - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a <code>Module</code>.</li>
<li><code>autograd.Function</code> - Implements forward and backward definitions of an autograd operation. Every <code>Tensor</code> operation creates at least a single <code>Function</code> node that connects to functions that created a <code>Tensor</code> and encodes its history.</li>
</ul>
<p>损失函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">output = net(input)</span><br><span class="line">target = torch.randn(10)  # a dummy target, for example</span><br><span class="line">target = target.view(1, -1)  # make it the same shape as output</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">print(loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor(1.1562, grad_fn=&lt;MseLossBackward&gt;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这时候，使用<code>.grad_fn</code>，可以看到一个计算的流程图：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d</span><br><span class="line">      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</span><br><span class="line">      -&gt; MSELoss</span><br><span class="line">      -&gt; loss</span><br></pre></td></tr></table></figure>

<p>因此，当我们调用<code>loss.backward()</code>时，则会计算梯度</p>
<p>为了说明backward流程，看下面的例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print(loss.grad_fn)  # MSELoss</span><br><span class="line">print(loss.grad_fn.next_functions[0][0])  # Linear</span><br><span class="line">print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">&lt;MseLossBackward object at 0x7fdba3216da0&gt;</span><br><span class="line">&lt;AddmmBackward object at 0x7fdba3216f28&gt;</span><br><span class="line">&lt;AccumulateGrad object at 0x7fdba3216f28&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Backprop:</p>
<p>反向传播使用<code>loss.backward()</code>即可，在这之前，需要清除之前存在的梯度，否则现有的梯度计算会累加到之前的梯度中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()     # zeroes the gradient buffers of all parameters</span><br><span class="line"></span><br><span class="line">print(&#x27;conv1.bias.grad before backward&#x27;)</span><br><span class="line">print(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">print(&#x27;conv1.bias.grad after backward&#x27;)</span><br><span class="line">print(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">conv1.bias.grad before backward</span><br><span class="line">tensor([0., 0., 0., 0., 0., 0.])</span><br><span class="line">conv1.bias.grad after backward</span><br><span class="line">tensor([-0.0002,  0.0045,  0.0017, -0.0099,  0.0092, -0.0044])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>更新weight：</p>
<p>pytorch提供了各种更新规则（比如：SGD，Nesterov—SGD，Adam，RMSProp，等）的包<code>torch.optim</code>。使用起来也非常建单：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># create your optimizer</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=0.01)</span><br><span class="line"></span><br><span class="line"># in your training loop:</span><br><span class="line">optimizer.zero_grad()   # zero the gradient buffers</span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()    # Does the update</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h1 id="TRAINING-A-CLASSIFIER"><a href="#TRAINING-A-CLASSIFIER" class="headerlink" title="TRAINING A CLASSIFIER"></a>TRAINING A CLASSIFIER</h1><p>一般地，当你处理数据的时候，比如图像，文本，语音，或者视频，你可以使用标准的python库加载数据，编程numpy array，然后转换numpy array 到 <code>torch.*Tensor</code></p>
<ul>
<li>For images, packages such as Pillow, OpenCV are useful</li>
<li>For audio, packages such as scipy and librosa</li>
<li>For text, either raw Python or Cython based loading, or NLTK and SpaCy are useful</li>
</ul>
<p>特别是对于vision，pytorch有一个包叫<code>torchvision</code>，可以便捷地加载通用的数据集，比如 Imagenet, CIFAR10, MNIST，</p>
<p>我们将使用CIFAR10的数据集，有airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. 这几类，图片的大小是3<em>32</em>32，其中3-channel color image of 32*32 pixels in size</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gg9bpvyihwj30rj0jl4qp.jpg" alt="捕获.PNG"></p>
<p>为了训练一个图片分类器，我们需要做一下几点：</p>
<ul>
<li>Load and normalizing the CIFAR10 training and test datasets using torchvision</li>
<li>Define a Convolutional Neural Network</li>
<li>Define a loss function</li>
<li>Train the network on the training data</li>
<li>Test the network on the test data</li>
</ul>
<h2 id="Loading-and-normalizing-CIFAR10"><a href="#Loading-and-normalizing-CIFAR10" class="headerlink" title="Loading and normalizing CIFAR10"></a>Loading and normalizing CIFAR10</h2><p>The output of torchvision datasets are PILImage images of range [0, 1]. We transform them to Tensors of normalized range [-1, 1]. .. note:</p>
<blockquote>
<p>If running on Windows and you get a BrokenPipeError, try setting the num_worker of torch.utils.data.DataLoader() to 0.</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">import torchvision.transforms as transforms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=True,</span><br><span class="line">                                        download=True, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,</span><br><span class="line">                                          shuffle=True, num_workers=2)</span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=False,</span><br><span class="line">                                       download=True, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=4,</span><br><span class="line">                                         shuffle=False, num_workers=2)</span><br><span class="line"></span><br><span class="line">classes = (&#x27;plane&#x27;, &#x27;car&#x27;, &#x27;bird&#x27;, &#x27;cat&#x27;,</span><br><span class="line">           &#x27;deer&#x27;, &#x27;dog&#x27;, &#x27;frog&#x27;, &#x27;horse&#x27;, &#x27;ship&#x27;, &#x27;truck&#x27;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out：</span><br><span class="line">Extracting ./data/cifar-10-python.tar.gz to ./data</span><br><span class="line">Files already downloaded and verified</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>下面的代表可以用来展示训练的图片</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># functions to show an image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def imshow(img):</span><br><span class="line">    img = img / 2 + 0.5     # unnormalize</span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (1, 2, 0)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># get some random training images</span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"># show images</span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"># print labels</span><br><span class="line">print(&#x27; &#x27;.join(&#x27;%5s&#x27; % classes[labels[j]] for j in range(4)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="Define-a-Convolutional-Neural-Network"><a href="#Define-a-Convolutional-Neural-Network" class="headerlink" title="Define a Convolutional Neural Network"></a>Define a Convolutional Neural Network</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Net(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(3, 6, 5)</span><br><span class="line">        self.pool = nn.MaxPool2d(2, 2)</span><br><span class="line">        self.conv2 = nn.Conv2d(6, 16, 5)</span><br><span class="line">        self.fc1 = nn.Linear(16 * 5 * 5, 120)</span><br><span class="line">        self.fc2 = nn.Linear(120, 84)</span><br><span class="line">        self.fc3 = nn.Linear(84, 10)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-1, 16 * 5 * 5)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Define-a-Loss-function-and-optimizer"><a href="#Define-a-Loss-function-and-optimizer" class="headerlink" title="Define a Loss function and optimizer"></a>Define a Loss function and optimizer</h2><p>使用的是 Cross-Entropy loss 和 SGD with momentum</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import torch.optim as optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h2 id="Train-the-network"><a href="#Train-the-network" class="headerlink" title="Train the network"></a>Train the network</h2><p>我们只需要循环我们的 data iterator，然后把input喂给网络即可</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">for epoch in range(2):  # loop over the dataset multiple times</span><br><span class="line"></span><br><span class="line">    running_loss = 0.0</span><br><span class="line">    for i, data in enumerate(trainloader, 0):</span><br><span class="line">        # get the inputs; data is a list of [inputs, labels]</span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        # zero the parameter gradients</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        # forward + backward + optimize</span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        # print statistics</span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        if i % 2000 == 1999:    # print every 2000 mini-batches</span><br><span class="line">            print(&#x27;[%d, %5d] loss: %.3f&#x27; %</span><br><span class="line">                  (epoch + 1, i + 1, running_loss / 2000))</span><br><span class="line">            running_loss = 0.0</span><br><span class="line"></span><br><span class="line">print(&#x27;Finished Training&#x27;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">[1,  2000] loss: 2.227</span><br><span class="line">[1,  4000] loss: 1.884</span><br><span class="line">[1,  6000] loss: 1.672</span><br><span class="line">[1,  8000] loss: 1.582</span><br><span class="line">[1, 10000] loss: 1.526</span><br><span class="line">[1, 12000] loss: 1.474</span><br><span class="line">[2,  2000] loss: 1.407</span><br><span class="line">[2,  4000] loss: 1.384</span><br><span class="line">[2,  6000] loss: 1.362</span><br><span class="line">[2,  8000] loss: 1.341</span><br><span class="line">[2, 10000] loss: 1.331</span><br><span class="line">[2, 12000] loss: 1.291</span><br><span class="line">Finished Training</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>保存我们的模型：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PATH = &#x27;./cifar_net.pth&#x27;</span><br><span class="line">torch.save(net.state_dict(), PATH)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Test-the-network-on-the-test-data"><a href="#Test-the-network-on-the-test-data" class="headerlink" title="Test the network on the test data"></a>Test the network on the test data</h2><p>我们可以先拿出几个测试集的样本 看看预测的怎么样</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">dataiter = iter(testloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"># print images</span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line">print(&#x27;GroundTruth: &#x27;, &#x27; &#x27;.join(&#x27;%5s&#x27; % classes[labels[j]] for j in range(4)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#加载训练好的模型</span><br><span class="line">net = Net()</span><br><span class="line">net.load_state_dict(torch.load(PATH))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#来看看模型预测的结果是什么</span><br><span class="line">outputs = net(images)</span><br><span class="line"></span><br><span class="line">_, predicted = torch.max(outputs, 1)</span><br><span class="line"></span><br><span class="line">print(&#x27;Predicted: &#x27;, &#x27; &#x27;.join(&#x27;%5s&#x27; % classes[predicted[j]]</span><br><span class="line">                              for j in range(4)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>我们可以看看在整个测试集上面训练的结果如何</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">correct = 0</span><br><span class="line">total = 0</span><br><span class="line">with torch.no_grad():</span><br><span class="line">    for data in testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs.data, 1)</span><br><span class="line">        total += labels.size(0)</span><br><span class="line">        correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">print(&#x27;Accuracy of the network on the 10000 test images: %d %%&#x27; % (</span><br><span class="line">    100 * correct / total))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>我们还可以看看哪些类模型识别的比较好，哪些类模型识别的比较差</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class_correct = list(0. for i in range(10))</span><br><span class="line">class_total = list(0. for i in range(10))</span><br><span class="line">with torch.no_grad():</span><br><span class="line">    for data in testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs, 1)</span><br><span class="line">        c = (predicted == labels).squeeze()</span><br><span class="line">        for i in range(4):</span><br><span class="line">            label = labels[i]</span><br><span class="line">            class_correct[label] += c[i].item()</span><br><span class="line">            class_total[label] += 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for i in range(10):</span><br><span class="line">    print(&#x27;Accuracy of %5s : %2d %%&#x27; % (</span><br><span class="line">        classes[i], 100 * class_correct[i] / class_total[i]))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="Training-on-GPU"><a href="#Training-on-GPU" class="headerlink" title="Training on GPU"></a>Training on GPU</h1><p>Just like how you transfer a Tensor onto the GPU, you transfer the neural net onto the GPU</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br><span class="line"></span><br><span class="line"># Assuming that we are on a CUDA machine, this should print a CUDA device:</span><br><span class="line"></span><br><span class="line">print(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">cuda:0</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>说明是在gpu设备上的</p>
<p>下面这些方法会遍历所有的模块，把参数和缓存都变成CUDA Tensor</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">net.to(device)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>记住你必须每一步都把input和label都变成GPU支持的</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">inputs, labels = data[0].to(device), data[1].to(device)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>后面还有一些好的例子，可以看官网：<br><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html">https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/06/29/pytorch%E7%9A%8460%E5%88%86%E9%92%9F%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="lowkeysp">
      <meta itemprop="description" content="lowkeysp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="lowkeysp' Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/29/pytorch%E7%9A%8460%E5%88%86%E9%92%9F%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">pytorch的60分钟学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-29 21:37:05" itemprop="dateCreated datePublished" datetime="2020-06-29T21:37:05+08:00">2020-06-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-05-11 17:25:03" itemprop="dateModified" datetime="2023-05-11T17:25:03+08:00">2023-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <meta name="referrer" content="no-referrer" />

<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a target="_blank" rel="noopener" href="http://pytorch123.com/SecondSection/what_is_pytorch/">http://pytorch123.com/SecondSection/what_is_pytorch/</a></p>
<h1 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h1><p>Tensors 类似于 NumPy 的 ndarrays ，同时 Tensors 可以使用 GPU 进行计算。</p>
<p>比如，我们构造一个5*3的矩阵，不初始化</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">&gt;&gt;&gt; x = torch.empty(5,3)</span><br><span class="line">&gt;&gt;&gt; x</span><br><span class="line">tensor([[-1.2141e-25,  4.5783e-41, -1.2141e-25],</span><br><span class="line">        [ 4.5783e-41,  1.3563e-19,  1.3563e-19],</span><br><span class="line">        [ 1.3563e-19,  1.3563e-19,  1.3563e-19],</span><br><span class="line">        [ 1.3563e-19,  6.1678e+16,  6.4890e-07],</span><br><span class="line">        [ 2.6176e-12,  4.0058e-11,  2.5787e-09]])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>构造一个随机初始化的矩阵：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.rand(5,3)</span><br><span class="line">tensor([[0.0276, 0.6181, 0.1152],</span><br><span class="line">        [0.2565, 0.2421, 0.5149],</span><br><span class="line">        [0.9311, 0.2453, 0.3041],</span><br><span class="line">        [0.6361, 0.8637, 0.1596],</span><br><span class="line">        [0.7435, 0.8903, 0.9017]])</span><br></pre></td></tr></table></figure>
<p>构造一个矩阵为全0，而且数据类型是long</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.zeros(5,3,dtype=torch.long)</span><br><span class="line">tensor([[0, 0, 0],</span><br><span class="line">        [0, 0, 0],</span><br><span class="line">        [0, 0, 0],</span><br><span class="line">        [0, 0, 0],</span><br><span class="line">        [0, 0, 0]])</span><br></pre></td></tr></table></figure>
<p>构造一个张量，直接使用数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.tensor([5.5,3])</span><br><span class="line">tensor([5.5000, 3.0000])</span><br></pre></td></tr></table></figure>
<p>创建一个<strong>基于一个已知的tensor</strong>的tensor</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = x.new_ones(5,3,dtype = torch.double)</span><br><span class="line">&gt;&gt;&gt; x</span><br><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]], dtype=torch.float64)</span><br><span class="line"></span><br><span class="line">#创建一个和x类似的y</span><br><span class="line">&gt;&gt;&gt; y = torch.randn_like(x,dtype=torch.float)</span><br><span class="line">&gt;&gt;&gt; y</span><br><span class="line">tensor([[ 0.0115,  0.2386,  0.6542],</span><br><span class="line">        [-0.1891, -0.6507,  1.0276],</span><br><span class="line">        [ 1.2823, -0.1774,  0.1024],</span><br><span class="line">        [-0.0743,  0.7157, -1.0440],</span><br><span class="line">        [-0.7562, -0.6935, -0.0515]])</span><br></pre></td></tr></table></figure>

<p>获得tensor的维度信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x.size()</span><br><span class="line">torch.Size([5, 3])</span><br></pre></td></tr></table></figure>

<p>加法操作</p>
<p>第一种：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x+y</span><br></pre></td></tr></table></figure>
<p>第二种</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.add(x,y)</span><br><span class="line"></span><br><span class="line">#使用下面方法，可以将结果付给result</span><br><span class="line">torch.add(x,y,out=result) </span><br></pre></td></tr></table></figure>
<p>第三种</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y.add_(x)</span><br><span class="line"></span><br><span class="line"># 解释： 把x加到y上去，类似于 y = x + y</span><br></pre></td></tr></table></figure>

<p>可以使用标准的Numpy类似的索引操作</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x[:,1]</span><br></pre></td></tr></table></figure>
<p>如果你想改变一个tensor的大小或者形状，可以使用torch.view</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = torch.randn(4,4)</span><br><span class="line">&gt;&gt;&gt; y = x.view(16)</span><br><span class="line">&gt;&gt;&gt; z = x.view(-1,8) #-1表示是另一个维度，即列向量</span><br><span class="line">&gt;&gt;&gt; x.size()</span><br><span class="line">torch.Size([4, 4])</span><br><span class="line">&gt;&gt;&gt; y.size()</span><br><span class="line">torch.Size([16])</span><br><span class="line">&gt;&gt;&gt; z.size()</span><br><span class="line">torch.Size([2, 8])</span><br></pre></td></tr></table></figure>

<p>如果你有一个元素tensor，可以使用.item()来获得这个value</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = torch.randn(1)</span><br><span class="line">&gt;&gt;&gt; x</span><br><span class="line">tensor([-0.2879])</span><br><span class="line">&gt;&gt;&gt; x.item()</span><br><span class="line">-0.28786054253578186</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Torch Tensor 和 Numpy Array 是使用的同样的内存位置（前提是Torch Tensor 是在CPU上运行的），更改其中的一个也会影响到另一个</p>
<p>把Torch Tensor 转换成 Numpy Array</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;a = torch.ones(5)</span><br><span class="line">&gt;&gt;print(a)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([1.,1.,1.,1.,1.,])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt; b = a.numpy()</span><br><span class="line">&gt;&gt; print(b)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">[1. 1. 1. 1. 1.]</span><br></pre></td></tr></table></figure>
<p>让我们看看改变其中一个，另一个是否也会改变吧</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; a.add_(1)</span><br><span class="line">&gt;&gt; print(a)</span><br><span class="line">&gt;&gt; print(b)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([2., 2., 2., 2., 2.])</span><br><span class="line">[2. 2. 2. 2. 2.]</span><br></pre></td></tr></table></figure>
<p>看到没，我们只是改变了a，但是b也相应发生了改变，因为他们是共享同一块内存的</p>
<p>把Numpy Array转换成Torch Tensor</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">a = np.ones(5)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, 1, out=a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">[2. 2. 2. 2. 2.]</span><br><span class="line">tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</span><br></pre></td></tr></table></figure>
<p>也是，改变Numpy Array也会改变torch tensor</p>
<p>在CPU上，除了CharTensor之外，其他所有的Tensor都可以转换成Numpy Array</p>
<p>可以使用<code>.to</code>方法让Tensor支持任意设备</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># let us run this cell only if CUDA is available</span><br><span class="line"># We will use ``torch.device`` objects to move tensors in and out of GPU</span><br><span class="line">if torch.cuda.is_available():</span><br><span class="line">    device = torch.device(&quot;cuda&quot;)          # a CUDA device object</span><br><span class="line">    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU</span><br><span class="line">    x = x.to(device)                       # or just use strings ``.to(&quot;cuda&quot;)``</span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(&quot;cpu&quot;, torch.double))       # ``.to`` can also change dtype together!</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([0.6469], device=&#x27;cuda:0&#x27;)</span><br><span class="line">tensor([0.6469], dtype=torch.float64)</span><br></pre></td></tr></table></figure>
<p>可以看到，第一个tensor是cuda的，第二个就是正常的。</p>
<h1 id="AUTOGRAD-AUTOMATIC-DIFFERENTIATION"><a href="#AUTOGRAD-AUTOMATIC-DIFFERENTIATION" class="headerlink" title="AUTOGRAD: AUTOMATIC DIFFERENTIATION"></a>AUTOGRAD: AUTOMATIC DIFFERENTIATION</h1><p><code>autograd</code>这个包可以提供自动梯度。</p>
<p>‘torch.Tensor’是这个包的核心类，如果你设置Tensor的’.requires_grad’为’True’的话，那么它会开始追踪你的所有的运算，进而可以算梯度。当你完成计算时，只需要调用’.backward()’,就可以自动地计算出所有的梯度。这个tensor的梯度会被累加在这个’.grad’属性值里</p>
<p>使用’.detach()’可以停止追踪计算</p>
<p>还可以使用代码块’with torch.no_grad()’来停止追踪计算。这在评估模型的时候会非常有用，因为评估模型的时候，模型可能会含有可训练的参数，且’.requires_grad &#x3D; True’，但是在评估模型的时候，并不需要梯度，因此可以使用这个代码块关闭追踪计算</p>
<p>‘Function’这个类在计算梯度的时候也是非常重要的。</p>
<p>每一个Tensor都有一个’.grad_fn’属性，用来指向创建这个Tensor的’Function’，除非这个Tensor在创建的时候，人为的设置’grad_fn’属性为’None’（下面的例子有解释）</p>
<p>举例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">x = torch.ones(2, 2, requires_grad=True)</span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([[1., 1.],</span><br><span class="line">        [1., 1.]], requires_grad=True)</span><br></pre></td></tr></table></figure>

<p>对这个x进行一次加法操作</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">y = x + 2</span><br><span class="line">print(y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line"></span><br><span class="line">tensor([[3., 3.],</span><br><span class="line">        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>


<p>这个y是通过加法运算得来的，因此他会有’grad_fn’属性</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(y.grad_fn)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">&lt;AddBackward0 object at 0x7f191afd60f0&gt;</span><br></pre></td></tr></table></figure>

<p>对y再进行运算</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">z = y * y * 3</span><br><span class="line">out = z.mean()</span><br><span class="line"></span><br><span class="line">print(z, out)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([[27., 27.],</span><br><span class="line">        [27., 27.]], grad_fn=&lt;MulBackward0&gt;) tensor(27., grad_fn=&lt;MeanBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p><code>.requires_grad_( ... )</code>可以改变Tensor的<code>requires_grad</code>属性。如果不指定的话，默认是False的。下面这个例子，刚开始a的<code>requires_grad</code>属性默认是False的，后来通过这个函数，可以改成True。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(2, 2)</span><br><span class="line">a = ((a * 3) / (a - 1))</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">a.requires_grad_(True)</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">b = (a * a).sum()</span><br><span class="line">print(b.grad_fn)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out：</span><br><span class="line">False</span><br><span class="line">True</span><br><span class="line">&lt;SumBackward0 object at 0x7f191afd6be0&gt;</span><br></pre></td></tr></table></figure>

<p>现在来计算Backprop,因为out是一个标量，所以<code>out.backward()</code>相当于是<code>out.backward(torch.tensor(1.))</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out.backward()</span><br></pre></td></tr></table></figure>

<p>打印d(out)&#x2F;dx 的梯度</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(x.grad)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([[4.5000, 4.5000],</span><br><span class="line">        [4.5000, 4.5000]])</span><br></pre></td></tr></table></figure>

<p>这个4.5的值是怎么得到的呢？</p>
<p>Tensor <code>out</code> 表示成$o$,我们有$o &#x3D; \frac{1}{4}\sum_i z_i$,$z_i &#x3D; 3(x_i+2)^2$,和$z_i\bigr\rvert_{x_i&#x3D;1} &#x3D; 27$。因此，有$\frac{\partial o}{\partial x_i} &#x3D; \frac{3}{2}(x_i+2)$,所以，$\frac{\partial o}{\partial x_i}\bigr\rvert_{x_i&#x3D;1} &#x3D; \frac{9}{2} &#x3D; 4.5$</p>
<p>在数学上，如果你有一个关于向量的函数，$\vec{y}&#x3D;f(\vec{x})$,那么关于$\vec{x}$的$\vec{y}$的梯度是一个Jacobian矩阵:</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gg8xlresm2j307f04gmx1.jpg" alt="捕获.PNG"></p>
<p>因此，可以说<code>torch.autograd</code>是用来计算vector-Jacobian product。也就是，给任意一个向量，$v&#x3D;\left(\begin{array}{cccc} v_{1} &amp; v_{2} &amp; \cdots &amp; v_{m}\end{array}\right)^{T}$,计算$v^{T}\cdot J$. 如果$v$刚好是函数$l&#x3D;g\left(\vec{y}\right)$的梯度，也就是$v&#x3D;\left(\begin{array}{ccc}\frac{\partial l}{\partial y_{1}} &amp; \cdots &amp; \frac{\partial l}{\partial y_{m}}\end{array}\right)^{T}$,那么根据chain rule，$v^{T}\cdot J$就是我们要求的$l$关于$\vec{x}$的梯度。（总结一下，就是如果要求梯度的话，如果我们有了Jacobian矩阵和向量$v$，那么我们就可以求出这个梯度）</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gg8xuj85rjj30bl04v3yi.jpg" alt="捕获.PNG"></p>
<p>我们来看一个例子</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(3, requires_grad=True)</span><br><span class="line"></span><br><span class="line">y = x * 2</span><br><span class="line">while y.data.norm() &lt; 1000:</span><br><span class="line">    y = y * 2</span><br><span class="line"></span><br><span class="line">print(y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([-970.9141,  465.0858, 1599.4425], grad_fn=&lt;MulBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>在这个例子里，$y$不是一个scalar，’torch.autograd’无法直接计算整个Jocobian，但是，如果我们只想要vector-Jacobian product的话，只需要把一个向量传给<code>backward</code>即可</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)</span><br><span class="line">y.backward(v)</span><br><span class="line"></span><br><span class="line">print(x.grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])</span><br></pre></td></tr></table></figure>


<p>当想要停止追踪计算时，如果Tensor的<code>requires_grad</code>属性是True时，可以使用<code>with torch.no_grad():</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">print(x.requires_grad)</span><br><span class="line">print((x ** 2).requires_grad)</span><br><span class="line"></span><br><span class="line">with torch.no_grad():</span><br><span class="line">    print((x ** 2).requires_grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">False</span><br></pre></td></tr></table></figure>

<p>还可以使用<code>.detach()</code>得到一个新的Tensor，这个Tensor和之前的Tensor是一样的，只是没有了<code>requires_grad=True</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print(x.requires_grad)</span><br><span class="line">y = x.detach()</span><br><span class="line">print(y.requires_grad)</span><br><span class="line">print(x.eq(y).all())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">True</span><br><span class="line">False</span><br><span class="line">tensor(True)</span><br></pre></td></tr></table></figure>

<p>参考一篇知乎讲解：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/65609544">https://zhuanlan.zhihu.com/p/65609544</a></p>
<h1 id="NEURAL-NETWORKS"><a href="#NEURAL-NETWORKS" class="headerlink" title="NEURAL NETWORKS"></a>NEURAL NETWORKS</h1><p>神经网络的搭建可以使用<code>torch.nn</code>包</p>
<p>一个’nn.Module’包括layers，一个<code>forward(input)</code>方法和一个<code>output</code></p>
<p>一个神经网络的训练过程如下：</p>
<ul>
<li>Define the neural network that has some learnable parameters (or weights)</li>
<li>Iterate over a dataset of inputs</li>
<li>Process input through the network</li>
<li>Compute the loss (how far is the output from being correct)</li>
<li>Propagate gradients back into the network’s parameters</li>
<li>Update the weights of the network, typically using a simple update rule: <code>weight = weight - learning_rate * gradient</code></li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gg8ztjlr3pj315f0cn77x.jpg" alt="捕获.PNG"></p>
<p>让我们定义这个神经网络：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Net(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        # 1 input image channel, 6 output channels, 3x3 square convolution</span><br><span class="line">        # kernel</span><br><span class="line">        self.conv1 = nn.Conv2d(1, 6, 3)</span><br><span class="line">        self.conv2 = nn.Conv2d(6, 16, 3)</span><br><span class="line">        # an affine operation: y = Wx + b</span><br><span class="line">        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension</span><br><span class="line">        self.fc2 = nn.Linear(120, 84)</span><br><span class="line">        self.fc3 = nn.Linear(84, 10)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # Max pooling over a (2, 2) window</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))</span><br><span class="line">        # If the size is a square you can only specify a single number</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), 2)</span><br><span class="line">        x = x.view(-1, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">    def num_flat_features(self, x):</span><br><span class="line">        size = x.size()[1:]  # all dimensions except the batch dimension</span><br><span class="line">        num_features = 1</span><br><span class="line">        for s in size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        return num_features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line"></span><br><span class="line">Net(</span><br><span class="line">  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))</span><br><span class="line">  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))</span><br><span class="line">  (fc1): Linear(in_features=576, out_features=120, bias=True)</span><br><span class="line">  (fc2): Linear(in_features=120, out_features=84, bias=True)</span><br><span class="line">  (fc3): Linear(in_features=84, out_features=10, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<p>你只需要定义<code>forward</code>函数,  <code>backward</code>函数会自动定义的（使用<code>autograd</code>）。你可以在<code>forward</code>函数里使用任意Tensor的operation</p>
<p>模型里面的可学习的参数可以通过<code>net.parameters()</code>学到</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">params = list(net.parameters())</span><br><span class="line">print(len(params))</span><br><span class="line">print(params[0].size())  # conv1&#x27;s .weight</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">10</span><br><span class="line">torch.Size([6, 1, 3, 3])</span><br></pre></td></tr></table></figure>

<p>让我们试一下32*32的input。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">input = torch.randn(1, 1, 32, 32)</span><br><span class="line">out = net(input)</span><br><span class="line">print(out)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([[ 0.1242,  0.1194, -0.0584, -0.1140,  0.0661,  0.0191, -0.0966,  0.0480,</span><br><span class="line">          0.0775, -0.0451]], grad_fn=&lt;AddmmBackward&gt;)</span><br></pre></td></tr></table></figure>

<p>之后清零 所有参数的梯度缓存 然后 使用backprops</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(1, 10))</span><br></pre></td></tr></table></figure>

<ul>
<li>NOTE:<blockquote>
<p>torch.nn only supports mini-batches. The entire torch.nn package only supports inputs that are a mini-batch of samples, and not a single sample.</p>
<p>For example, nn.Conv2d will take in a 4D Tensor of nSamples x nChannels x Height x Width.</p>
<p>If you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension.</p>
</blockquote>
</li>
</ul>
<p>回顾一下学到的：</p>
<ul>
<li><code>torch.Tensor</code> - A multi-dimensional array with support for autograd operations like <code>backward()</code>. Also holds the gradient w.r.t. the tensor.</li>
<li><code>nn.Module</code> - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.</li>
<li><code>nn.Parameter</code> - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a <code>Module</code>.</li>
<li><code>autograd.Function</code> - Implements forward and backward definitions of an autograd operation. Every <code>Tensor</code> operation creates at least a single <code>Function</code> node that connects to functions that created a <code>Tensor</code> and encodes its history.</li>
</ul>
<p>损失函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">output = net(input)</span><br><span class="line">target = torch.randn(10)  # a dummy target, for example</span><br><span class="line">target = target.view(1, -1)  # make it the same shape as output</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">print(loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor(1.1562, grad_fn=&lt;MseLossBackward&gt;)</span><br></pre></td></tr></table></figure>

<p>这时候，使用’.grad_fn’，可以看到一个计算的流程图：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d</span><br><span class="line">      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</span><br><span class="line">      -&gt; MSELoss</span><br><span class="line">      -&gt; loss</span><br></pre></td></tr></table></figure>

<p>因此，当我们调用<code>loss.backward()</code>时，则会计算梯度</p>
<p>为了说明backward流程，看下面的例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">print(loss.grad_fn)  # MSELoss</span><br><span class="line">print(loss.grad_fn.next_functions[0][0])  # Linear</span><br><span class="line">print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">&lt;MseLossBackward object at 0x7fdba3216da0&gt;</span><br><span class="line">&lt;AddmmBackward object at 0x7fdba3216f28&gt;</span><br><span class="line">&lt;AccumulateGrad object at 0x7fdba3216f28&gt;</span><br></pre></td></tr></table></figure>

<p>Backprop:</p>
<p>反向传播使用<code>loss.backward()</code>即可，在这之前，需要清除之前存在的梯度，否则现有的梯度计算会累加到之前的梯度种</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()     # zeroes the gradient buffers of all parameters</span><br><span class="line"></span><br><span class="line">print(&#x27;conv1.bias.grad before backward&#x27;)</span><br><span class="line">print(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">print(&#x27;conv1.bias.grad after backward&#x27;)</span><br><span class="line">print(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">conv1.bias.grad before backward</span><br><span class="line">tensor([0., 0., 0., 0., 0., 0.])</span><br><span class="line">conv1.bias.grad after backward</span><br><span class="line">tensor([-0.0002,  0.0045,  0.0017, -0.0099,  0.0092, -0.0044])</span><br></pre></td></tr></table></figure>

<p>更新weight：</p>
<p>pytorch提供了各种更新规则（比如：SGD，Nesterov—SGD，Adam，RMSProp，等）的包<code>torch.optim</code>。使用起来也非常建单：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># create your optimizer</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=0.01)</span><br><span class="line"></span><br><span class="line"># in your training loop:</span><br><span class="line">optimizer.zero_grad()   # zero the gradient buffers</span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()    # Does the update</span><br></pre></td></tr></table></figure>



<h1 id="TRAINING-A-CLASSIFIER"><a href="#TRAINING-A-CLASSIFIER" class="headerlink" title="TRAINING A CLASSIFIER"></a>TRAINING A CLASSIFIER</h1><p>一般地，当你处理数据的时候，比如图像，文本，语音，或者视频，你可以使用标准的python库加载数据，编程numpy array，然后转换numpy array 到 <code>torch.*Tensor</code></p>
<ul>
<li>For images, packages such as Pillow, OpenCV are useful</li>
<li>For audio, packages such as scipy and librosa</li>
<li>For text, either raw Python or Cython based loading, or NLTK and SpaCy are useful</li>
</ul>
<p>特别是对于vision，pytorch有一个包叫<code>torchvision</code>，可以便捷地加载通用的数据集，比如 Imagenet, CIFAR10, MNIST，</p>
<p>我们将使用CIFAR10的数据集，有‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. 这几类，图片的大小是3<em>32</em>32，其中3-channel color image of 32*32 pixels in size</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gg9bpvyihwj30rj0jl4qp.jpg" alt="捕获.PNG"></p>
<p>为了训练一个图片分类器，我们需要做一下几点：</p>
<ul>
<li>Load and normalizing the CIFAR10 training and test datasets using torchvision</li>
<li>Define a Convolutional Neural Network</li>
<li>Define a loss function</li>
<li>Train the network on the training data</li>
<li>Test the network on the test data</li>
</ul>
<h2 id="Loading-and-normalizing-CIFAR10"><a href="#Loading-and-normalizing-CIFAR10" class="headerlink" title="Loading and normalizing CIFAR10"></a>Loading and normalizing CIFAR10</h2><p>The output of torchvision datasets are PILImage images of range [0, 1]. We transform them to Tensors of normalized range [-1, 1]. .. note:</p>
<blockquote>
<p>If running on Windows and you get a BrokenPipeError, try setting the num_worker of torch.utils.data.DataLoader() to 0.</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">import torchvision.transforms as transforms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=True,</span><br><span class="line">                                        download=True, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,</span><br><span class="line">                                          shuffle=True, num_workers=2)</span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=False,</span><br><span class="line">                                       download=True, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=4,</span><br><span class="line">                                         shuffle=False, num_workers=2)</span><br><span class="line"></span><br><span class="line">classes = (&#x27;plane&#x27;, &#x27;car&#x27;, &#x27;bird&#x27;, &#x27;cat&#x27;,</span><br><span class="line">           &#x27;deer&#x27;, &#x27;dog&#x27;, &#x27;frog&#x27;, &#x27;horse&#x27;, &#x27;ship&#x27;, &#x27;truck&#x27;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out：</span><br><span class="line">Extracting ./data/cifar-10-python.tar.gz to ./data</span><br><span class="line">Files already downloaded and verified</span><br></pre></td></tr></table></figure>

<p>下面的代表可以用来展示训练的图片</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># functions to show an image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def imshow(img):</span><br><span class="line">    img = img / 2 + 0.5     # unnormalize</span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (1, 2, 0)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># get some random training images</span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"># show images</span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"># print labels</span><br><span class="line">print(&#x27; &#x27;.join(&#x27;%5s&#x27; % classes[labels[j]] for j in range(4)))</span><br></pre></td></tr></table></figure>



<h2 id="Define-a-Convolutional-Neural-Network"><a href="#Define-a-Convolutional-Neural-Network" class="headerlink" title="Define a Convolutional Neural Network"></a>Define a Convolutional Neural Network</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Net(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(3, 6, 5)</span><br><span class="line">        self.pool = nn.MaxPool2d(2, 2)</span><br><span class="line">        self.conv2 = nn.Conv2d(6, 16, 5)</span><br><span class="line">        self.fc1 = nn.Linear(16 * 5 * 5, 120)</span><br><span class="line">        self.fc2 = nn.Linear(120, 84)</span><br><span class="line">        self.fc3 = nn.Linear(84, 10)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-1, 16 * 5 * 5)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure>

<h2 id="Define-a-Loss-function-and-optimizer"><a href="#Define-a-Loss-function-and-optimizer" class="headerlink" title="Define a Loss function and optimizer"></a>Define a Loss function and optimizer</h2><p>使用的是 Cross-Entropy loss 和 SGD with momentum</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import torch.optim as optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h2 id="Train-the-network"><a href="#Train-the-network" class="headerlink" title="Train the network"></a>Train the network</h2><p>我们只需要循环我们的 data iterator，然后把input喂给网络即可</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">for epoch in range(2):  # loop over the dataset multiple times</span><br><span class="line"></span><br><span class="line">    running_loss = 0.0</span><br><span class="line">    for i, data in enumerate(trainloader, 0):</span><br><span class="line">        # get the inputs; data is a list of [inputs, labels]</span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        # zero the parameter gradients</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        # forward + backward + optimize</span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        # print statistics</span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        if i % 2000 == 1999:    # print every 2000 mini-batches</span><br><span class="line">            print(&#x27;[%d, %5d] loss: %.3f&#x27; %</span><br><span class="line">                  (epoch + 1, i + 1, running_loss / 2000))</span><br><span class="line">            running_loss = 0.0</span><br><span class="line"></span><br><span class="line">print(&#x27;Finished Training&#x27;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">[1,  2000] loss: 2.227</span><br><span class="line">[1,  4000] loss: 1.884</span><br><span class="line">[1,  6000] loss: 1.672</span><br><span class="line">[1,  8000] loss: 1.582</span><br><span class="line">[1, 10000] loss: 1.526</span><br><span class="line">[1, 12000] loss: 1.474</span><br><span class="line">[2,  2000] loss: 1.407</span><br><span class="line">[2,  4000] loss: 1.384</span><br><span class="line">[2,  6000] loss: 1.362</span><br><span class="line">[2,  8000] loss: 1.341</span><br><span class="line">[2, 10000] loss: 1.331</span><br><span class="line">[2, 12000] loss: 1.291</span><br><span class="line">Finished Training</span><br></pre></td></tr></table></figure>

<p>保存我们的模型：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PATH = &#x27;./cifar_net.pth&#x27;</span><br><span class="line">torch.save(net.state_dict(), PATH)</span><br></pre></td></tr></table></figure>

<h2 id="Test-the-network-on-the-test-data"><a href="#Test-the-network-on-the-test-data" class="headerlink" title="Test the network on the test data"></a>Test the network on the test data</h2><p>我们可以先拿出几个测试集的样本 看看预测的怎么样</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">dataiter = iter(testloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"># print images</span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line">print(&#x27;GroundTruth: &#x27;, &#x27; &#x27;.join(&#x27;%5s&#x27; % classes[labels[j]] for j in range(4)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#加载训练好的模型</span><br><span class="line">net = Net()</span><br><span class="line">net.load_state_dict(torch.load(PATH))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#来看看模型预测的结果是什么</span><br><span class="line">outputs = net(images)</span><br><span class="line"></span><br><span class="line">_, predicted = torch.max(outputs, 1)</span><br><span class="line"></span><br><span class="line">print(&#x27;Predicted: &#x27;, &#x27; &#x27;.join(&#x27;%5s&#x27; % classes[predicted[j]]</span><br><span class="line">                              for j in range(4)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>我们可以看看在整个测试集上面训练的结果如何</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">correct = 0</span><br><span class="line">total = 0</span><br><span class="line">with torch.no_grad():</span><br><span class="line">    for data in testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs.data, 1)</span><br><span class="line">        total += labels.size(0)</span><br><span class="line">        correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">print(&#x27;Accuracy of the network on the 10000 test images: %d %%&#x27; % (</span><br><span class="line">    100 * correct / total))</span><br></pre></td></tr></table></figure>

<p>我们还可以看看哪些类模型识别的比较好，哪些类模型识别的比较差</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class_correct = list(0. for i in range(10))</span><br><span class="line">class_total = list(0. for i in range(10))</span><br><span class="line">with torch.no_grad():</span><br><span class="line">    for data in testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs, 1)</span><br><span class="line">        c = (predicted == labels).squeeze()</span><br><span class="line">        for i in range(4):</span><br><span class="line">            label = labels[i]</span><br><span class="line">            class_correct[label] += c[i].item()</span><br><span class="line">            class_total[label] += 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for i in range(10):</span><br><span class="line">    print(&#x27;Accuracy of %5s : %2d %%&#x27; % (</span><br><span class="line">        classes[i], 100 * class_correct[i] / class_total[i]))</span><br></pre></td></tr></table></figure>

<h1 id="Training-on-GPU"><a href="#Training-on-GPU" class="headerlink" title="Training on GPU"></a>Training on GPU</h1><p>Just like how you transfer a Tensor onto the GPU, you transfer the neural net onto the GPU</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br><span class="line"></span><br><span class="line"># Assuming that we are on a CUDA machine, this should print a CUDA device:</span><br><span class="line"></span><br><span class="line">print(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">cuda:0</span><br></pre></td></tr></table></figure>
<p>说明是在gpu设备上的</p>
<p>下面这些方法会遍历所有的模块，把参数和缓存都变成CUDA Tensor</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.to(device)</span><br></pre></td></tr></table></figure>

<p>记住你必须每一步都把input和label都变成GPU支持的</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs, labels = data[0].to(device), data[1].to(device)</span><br></pre></td></tr></table></figure>


<p>后面还有一些好的例子，可以看官网：<br><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html">https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/04/07/LDP%E6%A0%87%E7%AD%BE%E5%88%86%E5%8F%91%E6%8E%A7%E5%88%B6%E6%A8%A1%E5%BC%8F%E3%80%81%E9%80%9A%E5%91%8A%E6%A8%A1%E5%BC%8F%E5%8F%8A%E4%BF%9D%E7%95%99%E6%A8%A1%E5%BC%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="lowkeysp">
      <meta itemprop="description" content="lowkeysp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="lowkeysp' Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/07/LDP%E6%A0%87%E7%AD%BE%E5%88%86%E5%8F%91%E6%8E%A7%E5%88%B6%E6%A8%A1%E5%BC%8F%E3%80%81%E9%80%9A%E5%91%8A%E6%A8%A1%E5%BC%8F%E5%8F%8A%E4%BF%9D%E7%95%99%E6%A8%A1%E5%BC%8F/" class="post-title-link" itemprop="url">LDP标签分发控制模式、通告模式及保留模式</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-07 18:32:17" itemprop="dateCreated datePublished" datetime="2020-04-07T18:32:17+08:00">2020-04-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-05-11 17:25:03" itemprop="dateModified" datetime="2023-05-11T17:25:03+08:00">2023-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">数据</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <meta name="referrer" content="no-referrer" />

<h1 id="标签通告模式"><a href="#标签通告模式" class="headerlink" title="标签通告模式"></a>标签通告模式</h1><p>标签通告模式 Label Advertisement Mode:在MPLS体系中，由下游LSR决定将标签分配给特定FEC，再通知上游LSR，即标签由下游指定，标签的分配按从下游到上游的方向分发。标签发布方式有两种方式。具有标签分发邻接关系的上游LSR和下游LSR必须对使用的标签发布方式达成一致。</p>
<ul>
<li>下游自主 Downstream Unsolicited（设备默认采用该方式）：不需要上游给自己发请求，主动给上游发标签</li>
<li>下游按需 Downstream On Demand ：不主动发，上游发请求之后才会给上游发标签</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gdlfivix33j30od0a3tar.jpg" alt="捕获.PNG"></p>
<h2 id="下游自主方式"><a href="#下游自主方式" class="headerlink" title="下游自主方式"></a>下游自主方式</h2><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gdlfjpi9ynj30n90ch78s.jpg" alt="捕获.PNG"></p>
<h2 id="下游按需方式"><a href="#下游按需方式" class="headerlink" title="下游按需方式"></a>下游按需方式</h2><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gdlfkq3808j30nh0baq6d.jpg" alt="捕获.PNG"></p>
<h1 id="标签分发控制模式"><a href="#标签分发控制模式" class="headerlink" title="标签分发控制模式"></a>标签分发控制模式</h1><p>标签分发控制模式 Label Distribution Control Mode:标签分发控制方式是指在LSP的建立过程中，LSR分配标签时采用的处理方式</p>
<ul>
<li>独立控制 Independent Control ：不需要等到下游给自己标签，然后捆绑后发给上游自己的标签</li>
<li>有序控制 Ordered Control（设备默认采用该方式）：需要等到下游给自己标签，然后捆绑后将自己的标签才发给上游</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gdlflv13ipj30nz06vdia.jpg" alt="捕获.PNG"></p>
<h2 id="独立控制"><a href="#独立控制" class="headerlink" title="独立控制"></a>独立控制</h2><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gdlfmxln8rj30or0c9jvr.jpg" alt="捕获.PNG"></p>
<h2 id="有序控制"><a href="#有序控制" class="headerlink" title="有序控制"></a>有序控制</h2><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gdlfnwuf5yj30od0chgro.jpg" alt="捕获.PNG"></p>
<h1 id="标签保留方式"><a href="#标签保留方式" class="headerlink" title="标签保留方式"></a>标签保留方式</h1><p>标签保留方式 Label Retention：标签保留方式是指LSR对收到的，但目前暂时不需要的标签映射的处理方式</p>
<ul>
<li>自由模式 Liberal Retention （设备默认采用该方式）：会把所有的标签映射都在本地保留起来</li>
<li>保守模式 Conservative Retention ：只会保存使用的标签值</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gdlfoiimkuj30no0bngoz.jpg" alt="捕获.PNG"></p>
<h2 id="自由模式"><a href="#自由模式" class="headerlink" title="自由模式"></a>自由模式</h2><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gdlfr8vckdj30nt0bh77p.jpg" alt="捕获.PNG"></p>
<h2 id="保守模式"><a href="#保守模式" class="headerlink" title="保守模式"></a>保守模式</h2><p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gdlfrr0o2lj30lb0c5n0x.jpg" alt="捕获.PNG"></p>
<h1 id="华为设备支持的组合模式"><a href="#华为设备支持的组合模式" class="headerlink" title="华为设备支持的组合模式"></a>华为设备支持的组合模式</h1><p>目前华为设备支持如下组合方式：</p>
<ul>
<li>第一种：下游自主方式 + 有序控制 + 自由模式 （该方式为缺省方式）</li>
<li>第二种： 下游按需方式 + 有序控制  +  保守模式</li>
</ul>
<h1 id="配置LDP标签发布方式"><a href="#配置LDP标签发布方式" class="headerlink" title="配置LDP标签发布方式"></a>配置LDP标签发布方式</h1><p>在接口视图下：</p>
<p>配置标签发布方式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mpls ldp advertisement &#123;dod | du&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>缺省情况下，标签发布模式为下游自主标签分发DU</li>
<li>邻居之间存在多链路的时候，所有接口的标签发布方式必须相同</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/04/07/MPLS%E9%9D%99%E6%80%81LSP%E7%9A%84%E9%85%8D%E7%BD%AE%E5%8F%8AMPLS%E8%BD%AC%E5%8F%91%E8%BF%87%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="lowkeysp">
      <meta itemprop="description" content="lowkeysp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="lowkeysp' Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/07/MPLS%E9%9D%99%E6%80%81LSP%E7%9A%84%E9%85%8D%E7%BD%AE%E5%8F%8AMPLS%E8%BD%AC%E5%8F%91%E8%BF%87%E7%A8%8B/" class="post-title-link" itemprop="url">MMPLS转发过程</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-07 16:30:47" itemprop="dateCreated datePublished" datetime="2020-04-07T16:30:47+08:00">2020-04-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-05-11 17:25:03" itemprop="dateModified" datetime="2023-05-11T17:25:03+08:00">2023-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">数据</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <meta name="referrer" content="no-referrer" />

<h1 id="MPLS详细转发过程"><a href="#MPLS详细转发过程" class="headerlink" title="MPLS详细转发过程"></a>MPLS详细转发过程</h1><ul>
<li><p>NHLFE（Next Hop Label Forwarding Entry，下一跳标签转发表项）用于指导MPLS报文的转发。</p>
<blockquote>
<p>NHLFE包括：Tunnel ID，出接口，下一跳，出标签，标签操作类型等信息。</p>
</blockquote>
</li>
<li><p>FTN （FEC-to-NHLFE,FEC到一组NHLFE的映射）</p>
<blockquote>
<p>通过查看FIB表中Tunnel ID值不为0x0的表项，能够获得FTN的详细信息。FTN只在Ingress存在。</p>
</blockquote>
</li>
<li><p>ILM（Incoming Label Map，入标签到一组下一跳标签转发表项的映射）</p>
<blockquote>
<p>ILM包括：Tunnel ID，入标签，入接口，标签操作类型等信息</p>
<p>ILM在Transit LSR的作用是将标签和NHLFE绑定。通过标签索引ILM表，就相当于使用目的IP地址查询FIB，能够得到所有的标签转发信息。</p>
</blockquote>
</li>
<li><p>Tunnel ID</p>
<blockquote>
<p>为了给使用隧道的上层应用（如VPN，路由管理）提供统一的接口，系统自动为隧道分配了一个ID，也称为Tunnel ID。该Tunnel ID的长度为32bit，只是本地有效。这是设备为各种隧道所分配的一个ID。在MPLS中，Tunnel ID还用于将FIB，ILM及NHLFE进行关联</p>
</blockquote>
</li>
</ul>
<h2 id="在报文转发过程中"><a href="#在报文转发过程中" class="headerlink" title="在报文转发过程中"></a>在报文转发过程中</h2><p>在Ingress LSR，通过查询FIB表和NHLFE表指导报文的转发</p>
<ul>
<li>当IP报文进入MPLS域时，首先查看FIB表，检查目的IP地址对应的Tunnel ID值是否为0x0.<blockquote>
<ul>
<li>如果Tunnel ID值为0x0，则进入正常的IP转发流程。</li>
<li>如果Tunnel ID值不为0x0，则进入MPLS转发流程</li>
</ul>
</blockquote>
</li>
</ul>
<p>在Transit LSR，通过查询ILM表和NHLFE表指导MPLS报文的换法。</p>
<p>在Egress LSR,通过查询ILM表指导MPLS报文的转发或查询路由表指导IP报文转发</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gdldk42f64j30o40cl0wr.jpg" alt="捕获.PNG"></p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gdldm58w87j30nm0codkt.jpg" alt="捕获.PNG"></p>
<p>R2是怎么知道这是一个标签报文呢？在L2数据链路层的报文头里面会有标识，表明后面跟的是标签还是IP报文头</p>
<p><img src="http://ww1.sinaimg.cn/large/006eDJDNly1gdldomw3kej30on0ckdkg.jpg" alt="捕获.PNG"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">lowkeysp</p>
  <div class="site-description" itemprop="description">lowkeysp</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">85</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">69</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lowkeysp</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
